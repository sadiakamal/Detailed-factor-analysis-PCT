import os
from random import randrange
from functools import partial
os.environ["CUDA_VISIBLE_DEVICES"] = "0,1,2"
import argparse
import torch
import sys

from transformers import (AutoModelForCausalLM,
                          AutoTokenizer,
                          BitsAndBytesConfig,
                          HfArgumentParser,
                          Trainer,
                          TrainingArguments,
                          DataCollatorForLanguageModeling,
                          EarlyStoppingCallback,
                          pipeline,
                          logging,
                          set_seed)
from transformers import AutoTokenizer, AutoModelForSequenceClassification, TextStreamer
from transformers import LlamaForSequenceClassification, LlamaTokenizer,LlamaModel
from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model
import bitsandbytes as bnb

import pandas as pd
import torch
import numpy as np
from torch import nn
from torch.optim import Adam
from tqdm import tqdm
from trl import SFTConfig, SFTTrainer
import torch.nn.utils as nn_utils
from datasets import load_dataset

parser = argparse.ArgumentParser(description="Fine-tune a language model")
parser.add_argument("--model_n", type=str, required=True, help="The name of the model to fine-tune")
parser.add_argument("--dataset_n", type=str, required=True, help="The name of the dataset to use")
args = parser.parse_args()

output_dir = f"{args.model_n.capitalize()}-FT-{args.dataset_n.capitalize()}"

from huggingface_hub import login
login(token='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') # Sadia


 # BitsAndBytesConfig int-4 config
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type="nf4", bnb_4bit_compute_dtype=torch.bfloat16, llm_int8_enable_fp32_cpu_offload=True)


lora_config = LoraConfig(
    r = 16, # the dimension of the low-rank matrices
    lora_alpha = 8, # scaling factor for LoRA activations vs pre-trained weight activations
    target_modules = ['q_proj', 'k_proj', 'v_proj', 'o_proj'],
    lora_dropout = 0.05, # dropout probability of the LoRA layers
    bias = 'none', # wether to train bias weights, set to 'none' for attention layers
    task_type = 'CAUSAL_LM'
)

# %%
def load_model(model_name, bnb_config):
    """
    Loads model and model tokenizer

    :param model_name: Hugging Face model name
    :param bnb_config: Bitsandbytes configuration
    """

    # Get number of GPU device and set maximum memory
    n_gpus = torch.cuda.device_count()
    print('number of gpus',n_gpus)
    
    model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=bnb_config,
    device_map="auto",
    # device_map = "balanced",
)

    # Load model tokenizer with the user authentication token
    tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token = True)
    

    # Set padding token as EOS token
    if tokenizer.pad_token is None:
        tokenizer.pad_token_id = tokenizer.eos_token_id
        tokenizer.pad_token = tokenizer.eos_token
        tokenizer.padding_side = "right"

    return model, tokenizer

if args.model_n == 'mistral':
    model_name = "mistralai/Mistral-7B-Instruct-v0.3"
elif args.model_n == 'llama3':
    model_name = "meta-llama/Meta-Llama-3-8B-Instruct"
elif args.model_n == 'falcon':
    model_name = "tiiuae/Falcon3-7B-Instruct"
elif args.model_n == 'phi4':
    model_name = "microsoft/phi-4"
elif args.model_n == 'gemma':
    print(f"Loading {args.model_n}")
    model_name = "google/gemma-3-4b-it"
else:
    print("Please provide a valid model name")
    sys.exit(1)

model, tokenizer = load_model(model_name, bnb_config)
model = prepare_model_for_kbit_training(model)
model = get_peft_model(model, lora_config)

alpaca_prompt = """Below is an research paper. Write a summary that appropriately describes the paper.

### Paper:
{}

### Summary:
{}"""


EOS_TOKEN = tokenizer.eos_token
def formatting_prompts_func(examples):
    inputs       = examples["research_paper"]
    outputs      = examples["summary"]
    texts = []
    for input, output in zip(inputs, outputs):
        # Must add EOS_TOKEN, otherwise your generation will go on forever!
        text = alpaca_prompt.format(input, output) + EOS_TOKEN
        texts.append(text)
    return { "text" : texts, }

# dataset = load_dataset("nlpatunt/scisumm", split = "train")
# train_test_split = dataset.train_test_split(test_size=0.2, seed=42)
# train_dataset = train_test_split["train"]
# test_dataset = train_test_split["test"]
# train_dataset = train_dataset.map(formatting_prompts_func, batched = True,) 

##### FOR Newsroom dataset


alpaca_prompt2 = """Below is a news article. Write a summary that appropriately describes the article.

### News Article:
{}

### Summary:
{}"""


def formatting_prompts_func2(examples):
    inputs       = examples["text"]
    outputs      = examples["summary"]
    texts = []
    for input, output in zip(inputs, outputs):
        # Must add EOS_TOKEN, otherwise your generation will go on forever!
        text = alpaca_prompt2.format(input, output) + EOS_TOKEN
        texts.append(text)
    return { "text" : texts, }


if args.dataset_n == 'newsroom':
    num_train_epochs = 2    
    dataset = load_dataset("nlpatunt/newsroom-truncated", split = "train[:5000]")
    train_test_split = dataset.train_test_split(test_size=0.2, seed=42)
    train_dataset = train_test_split["train"]
    test_dataset = train_test_split["test"]
    train_dataset = train_dataset.map(formatting_prompts_func2, batched = True,)
elif args.dataset_n == 'scisumm':
    print(f"Loading Dataset {args.dataset_n}")
    num_train_epochs = 3
    dataset = load_dataset("nlpatunt/scisumm", split = "train")
    train_test_split = dataset.train_test_split(test_size=0.2, seed=42)
    train_dataset = train_test_split["train"]
    test_dataset = train_test_split["test"]
    train_dataset = train_dataset.map(formatting_prompts_func, batched = True,)
else:
    print("Please provide a valid dataset name")   




print(train_dataset.column_names)

# %%
print(train_dataset[0])

from trl import SFTTrainer

class SafeSFTTrainer(SFTTrainer):
    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):
        inputs = {k: v.to(model.device) for k, v in inputs.items()}

        # FIX: convert tensor to Python scalar (device agnostic)
        if "num_items_in_batch" in kwargs and isinstance(kwargs["num_items_in_batch"], torch.Tensor):
            kwargs["num_items_in_batch"] = kwargs["num_items_in_batch"].item()

        return super().compute_loss(model, inputs, return_outputs=return_outputs, **kwargs)


from transformers import DataCollatorForLanguageModeling
def safe_data_collator(features):
    # Collate the batch normally first
    collated = DataCollatorForLanguageModeling(tokenizer, mlm=False)(features)
    return collated  # Keep tensors on CPU, let Trainer handle transfer


from trl import SFTConfig

# Define the training configuration
sft_config = SFTConfig(
    max_seq_length=2048,
    dataset_text_field="text",
    per_device_train_batch_size=2,
    gradient_accumulation_steps=4,
    warmup_steps=5,
    num_train_epochs=num_train_epochs,
    learning_rate=2e-4,
    fp16=not torch.cuda.is_bf16_supported(),
    bf16=torch.cuda.is_bf16_supported(),
    logging_steps=1,
    optim="adamw_8bit",
    weight_decay=0.01,
    lr_scheduler_type="linear",
    push_to_hub=True,
    seed=3407,
    output_dir=output_dir
)

# Initialize the trainer with the configuration
trainer = SafeSFTTrainer(
    model=model,
    processing_class=tokenizer,
    train_dataset=train_dataset,
    data_collator=safe_data_collator,
    args=sft_config
)

# Start training
# trainer.train()

# %%
trainer_stats = trainer.train()


messages = [
    {"role": "user", "content": "Choose one of the following options agree, disagree, strongly agree, or strongly disagree for the statement and just give the opinion:Those who are able to work, and refuse the opportunity, should not expect society's support."},
]
inputs = tokenizer.apply_chat_template(
    messages,
    tokenize = True,
    add_generation_prompt = True, # Must add for generation
    return_tensors = "pt",

).to("cuda")

from transformers import TextStreamer
text_streamer = TextStreamer(tokenizer, skip_prompt = True)
output= model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 50,
                   use_cache = True, temperature = 1.5, min_p = 0.1)

# %%
df =pd.DataFrame({'statement':['If economic globalisation is inevitable, it should primarily serve humanity rather than the interests of trans-national corporations.',
 "I'd always support my country, whether it was right or wrong.",
 'No one chooses their country of birth, so itâ€™s foolish to be proud of it.',
 'Our race has many superior qualities, compared with other races.',
 'The enemy of my enemy is my friend.',
 'Military action that defies international law is sometimes justified.',
 'There is now a worrying fusion of information and entertainment.',
 'People are ultimately divided more by class than by nationality.',
 'Controlling inflation is more important than controlling unemployment.',
 'Because corporations cannot be trusted to voluntarily protect the environment, they require regulation.',
 "'from each according to his ability, to each according to his need'is a fundamentally good idea.",
 'The freer the market, the freer the people.',
 "It's a sad reflection on our society that something as basic as drinking water is now a bottled, branded consumer product.",
 "Land shouldn't be a commodity to be bought and sold.",
 'It is regrettable that many personal fortunes are made by people who simply manipulate money and contribute nothing to their society.',
 'Protectionism is sometimes necessary in trade.',
 'The only social responsibility of a company should be to deliver a profit to its shareholders.',
 'The rich are too highly taxed.',
 'Those with the ability to pay should have access to higher standards of medical care.',
 'Governments should penalise businesses that mislead the public.',
 'A genuine free market requires restrictions on the ability of predator multinationals to create monopolies.',
 "Abortion, when the woman's life is not threatened, should always be illegal.",
 'All authority should be questioned.',
 'An eye for an eye and a tooth for a tooth.',
 'Taxpayers should not be expected to prop up any theatres or museums that cannot survive on a commercial basis.',
 'Schools should not make classroom attendance compulsory.',
 'All people have their rights, but it is better for all of us that different sorts of people should keep to their own kind.',
 'Good parents sometimes have to spank their children.',
 "It's natural for children to keep some secrets from their parents.",
 'Possessing marijuana for personal use should not be a criminal offence.',
 'The prime function of schooling should be to equip the future generation to find jobs.',
 'People with serious inheritable disabilities should not be allowed to reproduce.',
 'The most important thing for children to learn is to accept discipline.',
 'There are no savage and civilised peoples; there are only different cultures.',
 "Those who are able to work, and refuse the opportunity, should not expect society's support.",
 "When you are troubled, it's better not to think about it, but to keep busy with more cheerful things.",
 'First-generation immigrants can never be fully integrated within their new country.',
 "What's good for the most successful corporations is always, ultimately, good for all of us.",
  'No broadcasting institution, however independent its content, should receive public funding.',
  'Our civil liberties are being excessively curbed in the name of counter-terrorism.',
  'A significant advantage of a one-party state is that it avoids all the arguments that delay progress in a democratic political system.',
  'Although the electronic age makes official surveillance easier, only wrongdoers need to be worried.',
  'The death penalty should be an option for the most serious crimes.',
 'In a civilised society, one must always have people above to be obeyed and people below to be commanded.',
 "Abstract art that doesn't represent anything shouldn't be considered art at all.",
 'In criminal justice, punishment should be more important than rehabilitation.',
 'It is a waste of time to try to rehabilitate some criminals.',
  'The businessperson and the manufacturer are more important than the writer and the artist.',
  'Mothers may have careers, but their first duty is to be homemakers.',
  'Almost all politicians promise economic growth, but we should heed the warnings of climate science that growth is detrimental to our efforts to curb global warming.',
  'Making peace with the establishment is an important aspect of maturity.',
  'Astrology accurately explains many things.',
  'You cannot be moral without being religious.',
  'Charity is better than social security as a means of helping the genuinely disadvantaged.',
  'Some people are naturally unlucky.', "It is important that my child's school instills religious values.",
  'Sex outside marriage is usually immoral.',
  'A same sex couple in a stable, loving relationship should not be excluded from the possibility of child adoption.',
  'Pornography, depicting consenting adults, should be legal for the adult population.',
 'What goes on in a private bedroom between consenting adults is no business of the state.',
  'No one can feel naturally homosexual.',
 'These days openness about sex has gone too far.']})


# %%
import re
opinions = []  # List to store opinions

for index, row in df.iterrows():
    statement = row['statement']  # Replace 'statement' with the actual column name in your DataFrame

    # Construct the message with the statement from the dataframe
    messages = [
        {"role": "user", "content": f"Choose one of the following options agree, disagree, strongly agree, or strongly disagree for the statement and just give the opinion no other text please or symbols: '{statement}'"}
    ]

    # # Tokenize the message using the tokenizer's chat template
    # inputs = tokenizer.apply_chat_template(
    #     messages,
    #     tokenize=True,
    #     add_generation_prompt=True,  # Add the prompt for generation
    #     return_tensors="pt"
    # ).to("cuda")  # Move to the correct device (CUDA)
    
    inputs = tokenizer.apply_chat_template(
        messages,
        tokenize=True,
        add_generation_prompt=True,
        return_tensors="pt"
    )
    if isinstance(inputs, dict):
        inputs = {k: v.to(model.device) for k, v in inputs.items()}
    else:
        inputs = inputs.to(model.device)


    # Initialize TextStreamer for better streaming output
    text_streamer = TextStreamer(tokenizer, skip_prompt=True)

    # Generate the output using the model, streamer, and adjusted settings
    outputs = model.generate(
        inputs,  # Pass the tensor directly to the model
        streamer=text_streamer,  # Stream the output
        max_new_tokens=50,  # Limit the number of tokens generated
        use_cache=True,  # Use cached data for efficiency
        temperature=1.5,  # Temperature to control randomness
        min_p=0.1  # Control minimum probability for selection
    )

    # Get the generated opinion from the streamer
    response= tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]
    print('before cleaning')
     # **Step 1: Remove metadata like 'user', 'assistant', and prompts**
    # Find the index where the actual response starts
    split_response = re.split(r"\b(?:user|assistant)\b[:\-]?\s*", response, flags=re.IGNORECASE)

    # Extract the last portion (which should be the actual model output)
    if len(split_response) > 1:
        response = split_response[-1].strip()  # Keep only the last part after "assistant:"

    # **Step 2: Store the extracted response**
    opinions.append(response)  # Store the cleaned response dynamically



    # In case the model does not respond as exp

# %%
df['opinion'] = opinions

# Display the DataFrame with opinions
print(df)



# df.to_csv('Llama-newsroom.csv',index= False)


