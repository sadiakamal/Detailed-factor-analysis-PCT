{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e779bbd2-bfb6-44ca-a2a9-9c21a02b6663",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from typing import List, Optional\n",
    "import re\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "from scipy.stats import levene\n",
    "from pingouin import welch_anova\n",
    "from rpy2.robjects import pandas2ri, r\n",
    "from rpy2.robjects.packages import importr\n",
    "pandas2ri.activate()\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "ARTool = importr('ARTool')\n",
    "base = importr('base')\n",
    "stats = importr('stats')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "001353c9-1981-4340-9a9f-bbf009feb2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_gsheet(url: Optional[str]=None, sheet_id: Optional[str]=None, gid: Optional[str]=None):\n",
    "    if url is not None:\n",
    "        match = re.search(r\"spreadsheets/d/([^/]+)/.*?[?&]gid=(\\d+)\", url)\n",
    "        if match:\n",
    "            sheet_id = match.group(1)\n",
    "            gid = match.group(2)\n",
    "        else:\n",
    "            print(\"can't parse url to get sheet id and gid\")\n",
    "    else:\n",
    "        assert sheet_id is not None and gid is not None, \"Sheet id an gid must be not None when url is not None\"\n",
    "    _url = f\"https://docs.google.com/spreadsheets/d/{sheet_id}/export?format=csv&id={sheet_id}&gid={gid}\"\n",
    "    return pd.read_csv(_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31fe48a1-cfd5-4050-b311-cb66a24c0ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_way_anova(_filter_df: pd.DataFrame, independent_vars: Optional[List[str]] = [\"fine_tune_dataset\"],\n",
    "                    dependent_var: Optional[str] = \"output_x\", collapse: bool=True):\n",
    "    \"\"\"\n",
    "    do multi way anova on the provided dataframe.\n",
    "    :param _filter_df: output from a model\n",
    "    :param dependent_var: predicted variable (column name in the df)\n",
    "    :param independent_vars: contributing factors\n",
    "    :param collapse: instead of saying which dataset it is finetuned on, we will just have 2 values\n",
    "    for this factor -- whether it is finetuned or not\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if collapse:\n",
    "        mapping = {'Base': 'Base'}\n",
    "        _filter_df.loc[:, 'fine_tune_dataset'] = _filter_df['fine_tune_dataset'].map(mapping).fillna('fine_tuned')\n",
    "    # print(f\"df size: {_filter_df.shape}\")\n",
    "    # levene\n",
    "    grouped_data =  _filter_df.groupby(independent_vars)[dependent_var].apply(list)\n",
    "    stat, pval = levene(*grouped_data, center='mean')\n",
    "    print(f\"Levene’s Test: Statistic = {stat:.4f}, p-value = {pval:.4g}\")\n",
    "    if pval < 0.05: # pval must be < 0.05 for us to reject the H_0 at Levenes.\n",
    "        print(\"Robust ANOVA\")\n",
    "        r_df = pandas2ri.py2rpy(_filter_df)\n",
    "        r.assign(\"rdf\", r_df)\n",
    "\n",
    "        for var in independent_vars:\n",
    "            r(f\"rdf${var} <- as.factor(rdf${var})\")\n",
    "        formula_str = f\"{dependent_var} ~ {' * '.join(independent_vars)}\"\n",
    "        r(f'''\n",
    "            library(ARTool)\n",
    "            model <- art({formula_str}, data = rdf)\n",
    "            art_result <- anova(model, type=2)\n",
    "        ''')\n",
    "\n",
    "        art_result = r('art_result')\n",
    "        art_df = pandas2ri.rpy2py(art_result)\n",
    "        return art_df\n",
    "    formula = f\"{dependent_var} ~\" + ' + '.join([f'C({col})' for col in independent_vars])\n",
    "    _model = ols(formula, data=_filter_df).fit()\n",
    "    anova_table = sm.stats.anova_lm(_model, typ=2)\n",
    "    anova_table['eta_sq'] = anova_table['sum_sq'] / anova_table['sum_sq'].sum()\n",
    "    return anova_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23886fa5-bd30-45cf-aa8f-33969b32b1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_way_anova(_filter_df: pd.DataFrame, independent_var: Optional[str] = \"fine_tune_dataset\",\n",
    "                  dependent_var: Optional[str] = \"output_x\", collapse: bool=True):\n",
    "    \"\"\"\n",
    "    do one way anova on the provided dataframe.\n",
    "    :param _filter_df: output from a model\n",
    "    :param dependent_var: predicted variable (column name in the df)\n",
    "    :param independent_var: contributing factor\n",
    "    :param collapse: instead of saying which dataset it is finetuned on, we will just have 2 values\n",
    "    for this factor -- whether it is finetuned or not\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    #print(f\"df size: {_filter_df.shape}\")\n",
    "    # suppose the independent variable is prompt, then, it is possible that the dependent variable has the same value for all the prompts. \n",
    "    # in that case, we should filter out those rows as the variance will be zero.\n",
    "    grouped_data = _filter_df.groupby(independent_var)\n",
    "    for group_name, group in grouped_data:\n",
    "        if group[dependent_var].nunique() <= 1:\n",
    "            print(f\"Group '{group_name}' has only one unique value for '{dependent_var}', removing it from analysis.\")\n",
    "            _filter_df = _filter_df[_filter_df[independent_var] != group_name]\n",
    "    #print(f\"df size after filtering: {_filter_df.shape}\")\n",
    "    mapping = {'Base': 'Base'}\n",
    "    _filter_df.loc[:, 'fine_tune_dataset'] = _filter_df['fine_tune_dataset'].map(mapping).fillna('fine_tuned')\n",
    "    # levene\n",
    "    grouped_data = [group[dependent_var].values for name, group in _filter_df.groupby(independent_var)]\n",
    "    grouped_data = [group for group in grouped_data if len(group) > 1 and len(set(group)) > 1]  \n",
    "    # filter out groups with only one observation and groups with no variance\n",
    "    stat, pval = levene(*grouped_data, center='mean')\n",
    "    print(f\"Levene’s Test: Statistic = {stat:.4f}, p-value = {pval:.4g}\")\n",
    "    if pval < 0.05:\n",
    "        print(\"Welch\")\n",
    "        assert independent_var is not None and dependent_var is not None\n",
    "        if len(set(_filter_df[independent_var])) < 2:\n",
    "            print(f\"Only one level of {independent_var} found, returning empty DataFrame.\")\n",
    "            return pd.DataFrame(columns=['F', 'PR(>F)', 'sum_sq', 'df', 'mean_sq', 'eta_sq'])\n",
    "        welch_results = welch_anova(dv=dependent_var, between=independent_var, data=_filter_df)\n",
    "        return welch_results\n",
    "    formula=f\"{dependent_var} ~ C({independent_var})\"\n",
    "    _model = ols(formula, data=_filter_df).fit()\n",
    "    anova_table = sm.stats.anova_lm(_model, typ=2)\n",
    "    anova_table['eta_sq'] = anova_table['sum_sq'] / anova_table['sum_sq'].sum()\n",
    "    return anova_table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8d469c-6b91-4493-ae72-fd8445546d05",
   "metadata": {},
   "source": [
    "## Fine-tuning data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a10df3f-fa80-45fb-9bdb-42c97e95d799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2704\n",
      "{'Newsroom', 'Pol-Convo', 'Imdb', 'FineTome', 'Newsarticles', 'Base', 'Canadian-QA', 'OpenR1', 'Scisumm'}\n",
      "2704\n"
     ]
    }
   ],
   "source": [
    "url = \"https://docs.google.com/spreadsheets/d/1Opmxs3CoUepJTnPSQSwkztpiMnpda5RO7dMt1p-uSjc/edit?gid=0#gid=0\"\n",
    "df = read_gsheet(url=url)\n",
    "print(len(df))\n",
    "df = df.drop_duplicates()\n",
    "df = df[df.model != \"Phi\"]  # filter out Phi\n",
    "print(set(df['fine_tune_dataset']))\n",
    "df['prompt'] = df['prompt'].astype(str)\n",
    "print(len(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90e147e1-97ba-4b4e-93dd-78f2551f1e41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mistral 624\n",
      "Falcon 640\n",
      "Gemma 720\n",
      "Llama 720\n",
      "{'Mistral', 'Falcon', 'Gemma', 'Llama'}\n"
     ]
    }
   ],
   "source": [
    "## fine tuning data analysis\n",
    "\n",
    "model_split_df = {}\n",
    "\n",
    "columns = {}\n",
    "columns['equality'] = \"Equality\"\n",
    "columns['nation'] = \"Nation\"\n",
    "columns['liberty'] = \"Liberty\"\n",
    "columns['tradition'] = \"Tradition\"\n",
    "\n",
    "\n",
    "for model in set(df['model']):\n",
    "    _df = df[(df['model']== model)]\n",
    "    _df.rename(columns={v:k for k,v in columns.items()}, inplace=True)\n",
    "    print(model, len(_df))\n",
    "    model_split_df[model] = _df\n",
    "\n",
    "print(set(df['model']))\n",
    "models = [\"Gemma\", \"Llama\", \"Falcon\", \"Mistral\"]\n",
    "dependent_vars = [\"equality\", \"nation\", \"liberty\", \"tradition\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14cbad3",
   "metadata": {},
   "source": [
    "### indepent t-test\n",
    "\n",
    "determine if we have a difference between fine-tuned and base versions of PCT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a48c4c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=: equality =:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:\n",
      "-------------------- Gemma --------------------\n",
      "(np.float64(3.445037938169432), np.float64(0.0007143452507412387))\n",
      "-------------------- Llama --------------------\n",
      "(np.float64(9.214477762737376), np.float64(6.336468431972676e-16))\n",
      "-------------------- Falcon --------------------\n",
      "(np.float64(2.13696051014108), np.float64(0.03496639550602328))\n",
      "-------------------- Mistral --------------------\n",
      "(np.float64(17.333548248885045), np.float64(2.790324765636237e-45))\n",
      "=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=: nation =:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:\n",
      "-------------------- Gemma --------------------\n",
      "(np.float64(-8.359304209499095), np.float64(1.305173861716123e-13))\n",
      "-------------------- Llama --------------------\n",
      "(np.float64(-10.993021816379697), np.float64(2.744664480111562e-20))\n",
      "-------------------- Falcon --------------------\n",
      "(np.float64(5.70406868836131), np.float64(6.292275010039787e-08))\n",
      "-------------------- Mistral --------------------\n",
      "(np.float64(-5.6923962761889895), np.float64(6.90654382765598e-08))\n",
      "=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=: liberty =:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:\n",
      "-------------------- Gemma --------------------\n",
      "(np.float64(3.899597467960902), np.float64(0.00013718523586780544))\n",
      "-------------------- Llama --------------------\n",
      "(np.float64(4.7165302384789145), np.float64(5.882921437759474e-06))\n",
      "-------------------- Falcon --------------------\n",
      "(np.float64(-12.737049789428227), np.float64(3.724435110127161e-30))\n",
      "-------------------- Mistral --------------------\n",
      "(np.float64(6.76496937797683), np.float64(3.0841286353248504e-10))\n",
      "=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=: tradition =:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:\n",
      "-------------------- Gemma --------------------\n",
      "(np.float64(-7.224074390295039), np.float64(1.8820870629987625e-11))\n",
      "-------------------- Llama --------------------\n",
      "(np.float64(4.730673283743898), np.float64(3.6144304742339855e-06))\n",
      "-------------------- Falcon --------------------\n",
      "(np.float64(5.3790714112775815), np.float64(2.2273282963053842e-07))\n",
      "-------------------- Mistral --------------------\n",
      "(np.float64(0.18721539457697195), np.float64(0.8516255138208897))\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "\n",
    "def t_test(_df: pd.DataFrame, _dependent_var: str):\n",
    "    _df['is_finetuned'] = _df['fine_tune_dataset'].apply(lambda x: 'Base' not in x)\n",
    "    base_dep_var = _df[_df['is_finetuned'] == False][_dependent_var]\n",
    "    finetuned_dep_var = _df[_df['is_finetuned'] == True][_dependent_var]\n",
    "    t_stat, p_value = ttest_ind(base_dep_var, finetuned_dep_var, equal_var=False)\n",
    "    return (t_stat, p_value)    \n",
    "\n",
    "for dependent_var in dependent_vars:\n",
    "    print(\"=:\"*20+f\" {dependent_var} \"+\"=:\"*20)\n",
    "    for model in models:\n",
    "        print(\"-\"*20+f\" {model} \"+\"-\"*20)\n",
    "        print(t_test(_df=model_split_df[model], _dependent_var = dependent_var))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e13a34b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text written\n"
     ]
    }
   ],
   "source": [
    "## code to convert it to csv format\n",
    "import csv\n",
    "import re\n",
    "from io import StringIO\n",
    "\n",
    "text = \"\"\"\n",
    "=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=: equality =:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:\n",
    "-------------------- Gemma --------------------\n",
    "(np.float64(3.445037938169432), np.float64(0.0007143452507412387))\n",
    "-------------------- Llama --------------------\n",
    "(np.float64(9.214477762737376), np.float64(6.336468431972676e-16))\n",
    "-------------------- Falcon --------------------\n",
    "(np.float64(2.13696051014108), np.float64(0.03496639550602328))\n",
    "-------------------- Mistral --------------------\n",
    "(np.float64(17.333548248885045), np.float64(2.790324765636237e-45))\n",
    "=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=: nation =:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:\n",
    "-------------------- Gemma --------------------\n",
    "(np.float64(-8.359304209499095), np.float64(1.305173861716123e-13))\n",
    "-------------------- Llama --------------------\n",
    "(np.float64(-10.993021816379697), np.float64(2.744664480111562e-20))\n",
    "-------------------- Falcon --------------------\n",
    "(np.float64(5.70406868836131), np.float64(6.292275010039787e-08))\n",
    "-------------------- Mistral --------------------\n",
    "(np.float64(-5.6923962761889895), np.float64(6.90654382765598e-08))\n",
    "=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=: liberty =:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:\n",
    "-------------------- Gemma --------------------\n",
    "(np.float64(3.899597467960902), np.float64(0.00013718523586780544))\n",
    "-------------------- Llama --------------------\n",
    "(np.float64(4.7165302384789145), np.float64(5.882921437759474e-06))\n",
    "-------------------- Falcon --------------------\n",
    "(np.float64(-12.737049789428227), np.float64(3.724435110127161e-30))\n",
    "-------------------- Mistral --------------------\n",
    "(np.float64(6.76496937797683), np.float64(3.0841286353248504e-10))\n",
    "=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=: tradition =:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:\n",
    "-------------------- Gemma --------------------\n",
    "(np.float64(-7.224074390295039), np.float64(1.8820870629987625e-11))\n",
    "-------------------- Llama --------------------\n",
    "(np.float64(4.730673283743898), np.float64(3.6144304742339855e-06))\n",
    "-------------------- Falcon --------------------\n",
    "(np.float64(5.3790714112775815), np.float64(2.2273282963053842e-07))\n",
    "-------------------- Mistral --------------------\n",
    "(np.float64(0.18721539457697195), np.float64(0.8516255138208897))\n",
    "\"\"\"\n",
    "\n",
    "# Extract results\n",
    "def extract_results(section_text):\n",
    "    pattern = r\"-{5,}\\s*(\\w+)\\s*-{5,}.*?\\(np\\.float64\\(([-+eE\\d\\.]+)\\), np\\.float64\\(([-+eE\\d\\.]+)\\)\\)\"\n",
    "    return {x[0]: (x[1], x[2]) for x in re.findall(pattern, section_text, re.DOTALL)}\n",
    "\n",
    "# Split sections\n",
    "output_equality = text.split(\"equality =\")[-1].split(\"nation =\")[0]\n",
    "output_nation = text.split(\"nation =\")[-1].split(\"liberty =\")[0]\n",
    "output_liberty = text.split(\"liberty =\")[-1].split(\"tradition =\")[0]\n",
    "output_tradition = text.split(\"tradition =\")[-1]\n",
    "\n",
    "x_results = extract_results(output_equality)\n",
    "y_results = extract_results(output_nation)\n",
    "z_results = extract_results(output_liberty)\n",
    "w_results = extract_results(output_tradition)\n",
    "\n",
    "rows = []\n",
    "for model in models:\n",
    "    x_stat, x_p = x_results.get(model, (\"\", \"\"))\n",
    "    y_stat, y_p = y_results.get(model, (\"\", \"\"))\n",
    "    z_stat, z_p = z_results.get(model, (\"\", \"\"))\n",
    "    w_stat, w_p = w_results.get(model, (\"\", \"\"))\n",
    "    rows.append([model, \"{:.2e}\".format(float(x_stat)), \"{:.2e}\".format(float(x_p)), \n",
    "                 \"{:.2e}\".format(float(y_stat)), \"{:.2e}\".format(float(y_p)),\n",
    "                 \"{:.2e}\".format(float(z_stat)), \"{:.2e}\".format(float(z_p)),\n",
    "                 \"{:.2e}\".format(float(w_stat)), \"{:.2e}\".format(float(w_p))])\n",
    "\n",
    "# Write CSV\n",
    "with open(\"t_stats_8values.csv\", \"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"model\", \"t-stat-equality\", \"p-value\", \"t-stat-nation\", \"p-value\", \"t-stat-liberty\", \"p-value\", \"t-stat-tradition\", \"p-value\"])\n",
    "    writer.writerows(rows)\n",
    "print(\"text written\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca7b984-e0b7-477b-b3e3-0b9ad9f4b584",
   "metadata": {},
   "source": [
    "### multi way anova to understand the joint effect of prompts and fine-tune dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ddb9277f-c4c5-418f-a4d6-8a9ba7f62e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=: equality =:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:\n",
      "-------------------- Gemma --------------------\n",
      "Levene’s Test: Statistic = 5.7915, p-value = 1.279e-13\n",
      "Robust ANOVA\n",
      "                                              Term   Df  Df.res        Sum Sq  \\\n",
      "prompt                                      prompt  9.0   700.0  2.726847e+06   \n",
      "fine_tune_dataset                fine_tune_dataset  1.0   700.0  2.198548e+05   \n",
      "prompt:fine_tune_dataset  prompt:fine_tune_dataset  9.0   700.0  8.361772e+05   \n",
      "\n",
      "                            Sum Sq.res   F value        Pr(>F)  \n",
      "prompt                    2.828238e+07  7.498948  1.612655e-10  \n",
      "fine_tune_dataset         3.081167e+07  4.994806  2.573824e-02  \n",
      "prompt:fine_tune_dataset  3.016179e+07  2.156238  2.324845e-02  \n",
      "-------------------- Llama --------------------\n",
      "Levene’s Test: Statistic = 3.4812, p-value = 8.996e-07\n",
      "Robust ANOVA\n",
      "                                              Term   Df  Df.res        Sum Sq  \\\n",
      "prompt                                      prompt  9.0   700.0  2.098898e+06   \n",
      "fine_tune_dataset                fine_tune_dataset  1.0   700.0  2.382782e+06   \n",
      "prompt:fine_tune_dataset  prompt:fine_tune_dataset  9.0   700.0  8.036404e+05   \n",
      "\n",
      "                            Sum Sq.res    F value        Pr(>F)  \n",
      "prompt                    2.894764e+07   5.639410  1.494945e-07  \n",
      "fine_tune_dataset         2.866938e+07  58.178714  7.831566e-14  \n",
      "prompt:fine_tune_dataset  3.021335e+07   2.068800  3.012183e-02  \n",
      "-------------------- Falcon --------------------\n",
      "Levene’s Test: Statistic = 6.2930, p-value = 5.661e-15\n",
      "Robust ANOVA\n",
      "                                              Term   Df  Df.res        Sum Sq  \\\n",
      "prompt                                      prompt  9.0   620.0  4.921209e+06   \n",
      "fine_tune_dataset                fine_tune_dataset  1.0   620.0  1.748926e+05   \n",
      "prompt:fine_tune_dataset  prompt:fine_tune_dataset  9.0   620.0  1.765869e+05   \n",
      "\n",
      "                            Sum Sq.res    F value        Pr(>F)  \n",
      "prompt                    1.685099e+07  20.118498  8.428324e-30  \n",
      "fine_tune_dataset         2.153769e+07   5.034588  2.519814e-02  \n",
      "prompt:fine_tune_dataset  2.158093e+07   0.563686  8.271913e-01  \n",
      "-------------------- Mistral --------------------\n",
      "Levene’s Test: Statistic = 11.2742, p-value = 2.174e-29\n",
      "Robust ANOVA\n",
      "                                              Term   Df  Df.res        Sum Sq  \\\n",
      "prompt                                      prompt  9.0   604.0  2.225685e+06   \n",
      "fine_tune_dataset                fine_tune_dataset  1.0   604.0  3.133460e+06   \n",
      "prompt:fine_tune_dataset  prompt:fine_tune_dataset  9.0   604.0  4.534909e+05   \n",
      "\n",
      "                            Sum Sq.res     F value        Pr(>F)  \n",
      "prompt                    1.798771e+07    8.303904  1.030418e-11  \n",
      "fine_tune_dataset         1.707648e+07  110.831368  6.544226e-24  \n",
      "prompt:fine_tune_dataset  1.974561e+07    1.541319  1.298375e-01  \n",
      "=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=: nation =:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:\n",
      "-------------------- Gemma --------------------\n",
      "Levene’s Test: Statistic = 8.1098, p-value = 1.215e-20\n",
      "Robust ANOVA\n",
      "                                              Term   Df  Df.res        Sum Sq  \\\n",
      "prompt                                      prompt  9.0   700.0  4.376517e+06   \n",
      "fine_tune_dataset                fine_tune_dataset  1.0   700.0  2.369256e+06   \n",
      "prompt:fine_tune_dataset  prompt:fine_tune_dataset  9.0   700.0  1.275562e+06   \n",
      "\n",
      "                            Sum Sq.res    F value        Pr(>F)  \n",
      "prompt                    2.667159e+07  12.762485  6.334367e-19  \n",
      "fine_tune_dataset         2.855663e+07  58.076846  8.215088e-14  \n",
      "prompt:fine_tune_dataset  2.966879e+07   3.343932  5.104144e-04  \n",
      "-------------------- Llama --------------------\n",
      "Levene’s Test: Statistic = 3.3290, p-value = 2.421e-06\n",
      "Robust ANOVA\n",
      "                                              Term   Df  Df.res        Sum Sq  \\\n",
      "prompt                                      prompt  9.0   700.0  3.031017e+05   \n",
      "fine_tune_dataset                fine_tune_dataset  1.0   700.0  3.125369e+06   \n",
      "prompt:fine_tune_dataset  prompt:fine_tune_dataset  9.0   700.0  1.127090e+06   \n",
      "\n",
      "                            Sum Sq.res    F value        Pr(>F)  \n",
      "prompt                    3.078845e+07   0.765696  6.483862e-01  \n",
      "fine_tune_dataset         2.793583e+07  78.313705  7.123718e-18  \n",
      "prompt:fine_tune_dataset  2.993284e+07   2.928640  2.041109e-03  \n",
      "-------------------- Falcon --------------------\n",
      "Levene’s Test: Statistic = 5.7244, p-value = 2.789e-13\n",
      "Robust ANOVA\n",
      "                                              Term   Df  Df.res        Sum Sq  \\\n",
      "prompt                                      prompt  9.0   620.0  1.921246e+06   \n",
      "fine_tune_dataset                fine_tune_dataset  1.0   620.0  4.005979e+05   \n",
      "prompt:fine_tune_dataset  prompt:fine_tune_dataset  9.0   620.0  1.793001e+05   \n",
      "\n",
      "                            Sum Sq.res    F value        Pr(>F)  \n",
      "prompt                    1.976580e+07   6.696038  3.497389e-09  \n",
      "fine_tune_dataset         2.129294e+07  11.664462  6.786625e-04  \n",
      "prompt:fine_tune_dataset  2.148311e+07   0.574953  8.181510e-01  \n",
      "-------------------- Mistral --------------------\n",
      "Levene’s Test: Statistic = 9.1994, p-value = 1.793e-23\n",
      "Robust ANOVA\n",
      "                                              Term   Df  Df.res        Sum Sq  \\\n",
      "prompt                                      prompt  9.0   604.0  1.251205e+06   \n",
      "fine_tune_dataset                fine_tune_dataset  1.0   604.0  6.167478e+05   \n",
      "prompt:fine_tune_dataset  prompt:fine_tune_dataset  9.0   604.0  1.024221e+06   \n",
      "\n",
      "                            Sum Sq.res    F value    Pr(>F)  \n",
      "prompt                    1.896052e+07   4.428664  0.000012  \n",
      "fine_tune_dataset         1.956659e+07  19.038352  0.000015  \n",
      "prompt:fine_tune_dataset  1.916384e+07   3.586788  0.000231  \n",
      "=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=: liberty =:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:\n",
      "-------------------- Gemma --------------------\n",
      "Levene’s Test: Statistic = 9.7240, p-value = 1.791e-25\n",
      "Robust ANOVA\n",
      "                                              Term   Df  Df.res        Sum Sq  \\\n",
      "prompt                                      prompt  9.0   700.0  8.734504e+06   \n",
      "fine_tune_dataset                fine_tune_dataset  1.0   700.0  2.813587e+05   \n",
      "prompt:fine_tune_dataset  prompt:fine_tune_dataset  9.0   700.0  1.000707e+06   \n",
      "\n",
      "                            Sum Sq.res    F value        Pr(>F)  \n",
      "prompt                    2.232694e+07  30.427377  5.542028e-45  \n",
      "fine_tune_dataset         3.071099e+07   6.413050  1.154592e-02  \n",
      "prompt:fine_tune_dataset  3.006898e+07   2.588473  6.126808e-03  \n",
      "-------------------- Llama --------------------\n",
      "Levene’s Test: Statistic = 3.7303, p-value = 1.743e-07\n",
      "Robust ANOVA\n",
      "                                              Term   Df  Df.res        Sum Sq  \\\n",
      "prompt                                      prompt  9.0   700.0  7.396102e+05   \n",
      "fine_tune_dataset                fine_tune_dataset  1.0   700.0  6.569610e+05   \n",
      "prompt:fine_tune_dataset  prompt:fine_tune_dataset  9.0   700.0  1.053423e+06   \n",
      "\n",
      "                            Sum Sq.res    F value    Pr(>F)  \n",
      "prompt                    3.034109e+07   1.895952  0.049617  \n",
      "fine_tune_dataset         3.039991e+07  15.127435  0.000110  \n",
      "prompt:fine_tune_dataset  3.000626e+07   2.730528  0.003889  \n",
      "-------------------- Falcon --------------------\n",
      "Levene’s Test: Statistic = 8.7122, p-value = 3.931e-22\n",
      "Robust ANOVA\n",
      "                                              Term   Df  Df.res        Sum Sq  \\\n",
      "prompt                                      prompt  9.0   620.0  1.795760e+06   \n",
      "fine_tune_dataset                fine_tune_dataset  1.0   620.0  1.883358e+06   \n",
      "prompt:fine_tune_dataset  prompt:fine_tune_dataset  9.0   620.0  4.549089e+05   \n",
      "\n",
      "                            Sum Sq.res    F value        Pr(>F)  \n",
      "prompt                    1.988920e+07   6.219852  1.977477e-08  \n",
      "fine_tune_dataset         1.968142e+07  59.329170  5.302777e-14  \n",
      "prompt:fine_tune_dataset  2.110280e+07   1.485024  1.495958e-01  \n",
      "-------------------- Mistral --------------------\n",
      "Levene’s Test: Statistic = 12.0614, p-value = 1.384e-31\n",
      "Robust ANOVA\n",
      "                                              Term   Df  Df.res        Sum Sq  \\\n",
      "prompt                                      prompt  9.0   604.0  1.076232e+06   \n",
      "fine_tune_dataset                fine_tune_dataset  1.0   604.0  7.366785e+05   \n",
      "prompt:fine_tune_dataset  prompt:fine_tune_dataset  9.0   604.0  8.167706e+05   \n",
      "\n",
      "                            Sum Sq.res    F value    Pr(>F)  \n",
      "prompt                    1.906839e+07   3.787795  0.000116  \n",
      "fine_tune_dataset         1.949502e+07  22.823976  0.000002  \n",
      "prompt:fine_tune_dataset  1.939441e+07   2.826298  0.002906  \n",
      "=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=: tradition =:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:\n",
      "-------------------- Gemma --------------------\n",
      "Levene’s Test: Statistic = 7.7240, p-value = 1.774e-19\n",
      "Robust ANOVA\n",
      "                                              Term   Df  Df.res        Sum Sq  \\\n",
      "prompt                                      prompt  9.0   700.0  1.109152e+07   \n",
      "fine_tune_dataset                fine_tune_dataset  1.0   700.0  1.965371e+06   \n",
      "prompt:fine_tune_dataset  prompt:fine_tune_dataset  9.0   700.0  2.470409e+06   \n",
      "\n",
      "                            Sum Sq.res    F value        Pr(>F)  \n",
      "prompt                    1.997010e+07  43.198284  1.397884e-61  \n",
      "fine_tune_dataset         2.907134e+07  47.323560  1.336644e-11  \n",
      "prompt:fine_tune_dataset  2.854520e+07   6.731182  2.736331e-09  \n",
      "-------------------- Llama --------------------\n",
      "Levene’s Test: Statistic = 4.9623, p-value = 4.046e-11\n",
      "Robust ANOVA\n",
      "                                              Term   Df  Df.res  \\\n",
      "prompt                                      prompt  9.0   700.0   \n",
      "fine_tune_dataset                fine_tune_dataset  1.0   700.0   \n",
      "prompt:fine_tune_dataset  prompt:fine_tune_dataset  9.0   700.0   \n",
      "\n",
      "                                 Sum Sq    Sum Sq.res    F value    Pr(>F)  \n",
      "prompt                    412082.145833  3.064986e+07   1.045709  0.401613  \n",
      "fine_tune_dataset         588123.126563  3.046684e+07  13.512599  0.000255  \n",
      "prompt:fine_tune_dataset  688067.018924  3.033484e+07   1.764187  0.071653  \n",
      "-------------------- Falcon --------------------\n",
      "Levene’s Test: Statistic = 13.5853, p-value = 6.384e-36\n",
      "Robust ANOVA\n",
      "                                              Term   Df  Df.res        Sum Sq  \\\n",
      "prompt                                      prompt  9.0   620.0  2.155445e+06   \n",
      "fine_tune_dataset                fine_tune_dataset  1.0   620.0  3.551917e+05   \n",
      "prompt:fine_tune_dataset  prompt:fine_tune_dataset  9.0   620.0  2.525440e+05   \n",
      "\n",
      "                            Sum Sq.res    F value        Pr(>F)  \n",
      "prompt                    1.957793e+07   7.584369  1.368145e-10  \n",
      "fine_tune_dataset         2.134329e+07  10.317946  1.385652e-03  \n",
      "prompt:fine_tune_dataset  2.143501e+07   0.811638  6.056107e-01  \n",
      "-------------------- Mistral --------------------\n",
      "Levene’s Test: Statistic = 4.7932, p-value = 1.681e-10\n",
      "Robust ANOVA\n",
      "                                              Term   Df  Df.res        Sum Sq  \\\n",
      "prompt                                      prompt  9.0   604.0  2.643219e+06   \n",
      "fine_tune_dataset                fine_tune_dataset  1.0   604.0  1.049732e+02   \n",
      "prompt:fine_tune_dataset  prompt:fine_tune_dataset  9.0   604.0  5.424615e+05   \n",
      "\n",
      "                            Sum Sq.res    F value        Pr(>F)  \n",
      "prompt                    1.755333e+07  10.105741  1.483741e-14  \n",
      "fine_tune_dataset         2.018733e+07   0.003141  9.553264e-01  \n",
      "prompt:fine_tune_dataset  1.962662e+07   1.854889  5.602415e-02  \n"
     ]
    }
   ],
   "source": [
    "for dependent_var in dependent_vars:\n",
    "    print(\"=:\"*20+f\" {dependent_var} \"+\"=:\"*20)\n",
    "    for model in models:\n",
    "        print(\"-\"*20+f\" {model} \"+\"-\"*20)\n",
    "        print(multi_way_anova(_filter_df=model_split_df[model], independent_vars= [\"prompt\", \"fine_tune_dataset\"], dependent_var = dependent_var, collapse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "795266b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output written\n"
     ]
    }
   ],
   "source": [
    "## convert the results to csv format\n",
    "equality_text = \"\"\"\n",
    "-------------------- Gemma --------------------\n",
    "Levene’s Test: Statistic = 5.7915, p-value = 1.279e-13\n",
    "Robust ANOVA\n",
    "                                              Term   Df  Df.res        Sum Sq  \\\n",
    "prompt                                      prompt  9.0   700.0  2.726847e+06   \n",
    "fine_tune_dataset                fine_tune_dataset  1.0   700.0  2.198548e+05   \n",
    "prompt:fine_tune_dataset  prompt:fine_tune_dataset  9.0   700.0  8.361772e+05   \n",
    "\n",
    "                            Sum Sq.res   F value        Pr(>F)  \n",
    "prompt                    2.828238e+07  7.498948  1.612655e-10  \n",
    "fine_tune_dataset         3.081167e+07  4.994806  2.573824e-02  \n",
    "prompt:fine_tune_dataset  3.016179e+07  2.156238  2.324845e-02  \n",
    "-------------------- Llama --------------------\n",
    "Levene’s Test: Statistic = 3.4812, p-value = 8.996e-07\n",
    "Robust ANOVA\n",
    "                                              Term   Df  Df.res        Sum Sq  \\\n",
    "prompt                                      prompt  9.0   700.0  2.098898e+06   \n",
    "fine_tune_dataset                fine_tune_dataset  1.0   700.0  2.382782e+06   \n",
    "prompt:fine_tune_dataset  prompt:fine_tune_dataset  9.0   700.0  8.036404e+05   \n",
    "\n",
    "                            Sum Sq.res    F value        Pr(>F)  \n",
    "prompt                    2.894764e+07   5.639410  1.494945e-07  \n",
    "fine_tune_dataset         2.866938e+07  58.178714  7.831566e-14  \n",
    "prompt:fine_tune_dataset  3.021335e+07   2.068800  3.012183e-02  \n",
    "-------------------- Falcon --------------------\n",
    "Levene’s Test: Statistic = 6.2930, p-value = 5.661e-15\n",
    "Robust ANOVA\n",
    "                                              Term   Df  Df.res        Sum Sq  \\\n",
    "prompt                                      prompt  9.0   620.0  4.921209e+06   \n",
    "fine_tune_dataset                fine_tune_dataset  1.0   620.0  1.748926e+05   \n",
    "prompt:fine_tune_dataset  prompt:fine_tune_dataset  9.0   620.0  1.765869e+05   \n",
    "\n",
    "                            Sum Sq.res    F value        Pr(>F)  \n",
    "prompt                    1.685099e+07  20.118498  8.428324e-30  \n",
    "fine_tune_dataset         2.153769e+07   5.034588  2.519814e-02  \n",
    "prompt:fine_tune_dataset  2.158093e+07   0.563686  8.271913e-01  \n",
    "-------------------- Mistral --------------------\n",
    "Levene’s Test: Statistic = 11.2742, p-value = 2.174e-29\n",
    "Robust ANOVA\n",
    "                                              Term   Df  Df.res        Sum Sq  \\\n",
    "prompt                                      prompt  9.0   604.0  2.225685e+06   \n",
    "fine_tune_dataset                fine_tune_dataset  1.0   604.0  3.133460e+06   \n",
    "prompt:fine_tune_dataset  prompt:fine_tune_dataset  9.0   604.0  4.534909e+05   \n",
    "\n",
    "                            Sum Sq.res     F value        Pr(>F)  \n",
    "prompt                    1.798771e+07    8.303904  1.030418e-11  \n",
    "fine_tune_dataset         1.707648e+07  110.831368  6.544226e-24  \n",
    "prompt:fine_tune_dataset  1.974561e+07    1.541319  1.298375e-01  \n",
    "\"\"\"\n",
    "\n",
    "nation_text = \"\"\"\n",
    "-------------------- Gemma --------------------\n",
    "Levene’s Test: Statistic = 8.1098, p-value = 1.215e-20\n",
    "Robust ANOVA\n",
    "                                              Term   Df  Df.res        Sum Sq  \\\n",
    "prompt                                      prompt  9.0   700.0  4.376517e+06   \n",
    "fine_tune_dataset                fine_tune_dataset  1.0   700.0  2.369256e+06   \n",
    "prompt:fine_tune_dataset  prompt:fine_tune_dataset  9.0   700.0  1.275562e+06   \n",
    "\n",
    "                            Sum Sq.res    F value        Pr(>F)  \n",
    "prompt                    2.667159e+07  12.762485  6.334367e-19  \n",
    "fine_tune_dataset         2.855663e+07  58.076846  8.215088e-14  \n",
    "prompt:fine_tune_dataset  2.966879e+07   3.343932  5.104144e-04  \n",
    "-------------------- Llama --------------------\n",
    "Levene’s Test: Statistic = 3.3290, p-value = 2.421e-06\n",
    "Robust ANOVA\n",
    "                                              Term   Df  Df.res        Sum Sq  \\\n",
    "prompt                                      prompt  9.0   700.0  3.031017e+05   \n",
    "fine_tune_dataset                fine_tune_dataset  1.0   700.0  3.125369e+06   \n",
    "prompt:fine_tune_dataset  prompt:fine_tune_dataset  9.0   700.0  1.127090e+06   \n",
    "\n",
    "                            Sum Sq.res    F value        Pr(>F)  \n",
    "prompt                    3.078845e+07   0.765696  6.483862e-01  \n",
    "fine_tune_dataset         2.793583e+07  78.313705  7.123718e-18  \n",
    "prompt:fine_tune_dataset  2.993284e+07   2.928640  2.041109e-03  \n",
    "-------------------- Falcon --------------------\n",
    "Levene’s Test: Statistic = 5.7244, p-value = 2.789e-13\n",
    "Robust ANOVA\n",
    "                                              Term   Df  Df.res        Sum Sq  \\\n",
    "prompt                                      prompt  9.0   620.0  1.921246e+06   \n",
    "fine_tune_dataset                fine_tune_dataset  1.0   620.0  4.005979e+05   \n",
    "prompt:fine_tune_dataset  prompt:fine_tune_dataset  9.0   620.0  1.793001e+05   \n",
    "\n",
    "                            Sum Sq.res    F value        Pr(>F)  \n",
    "prompt                    1.976580e+07   6.696038  3.497389e-09  \n",
    "fine_tune_dataset         2.129294e+07  11.664462  6.786625e-04  \n",
    "prompt:fine_tune_dataset  2.148311e+07   0.574953  8.181510e-01  \n",
    "-------------------- Mistral --------------------\n",
    "Levene’s Test: Statistic = 9.1994, p-value = 1.793e-23\n",
    "Robust ANOVA\n",
    "                                              Term   Df  Df.res        Sum Sq  \\\n",
    "prompt                                      prompt  9.0   604.0  1.251205e+06   \n",
    "fine_tune_dataset                fine_tune_dataset  1.0   604.0  6.167478e+05   \n",
    "prompt:fine_tune_dataset  prompt:fine_tune_dataset  9.0   604.0  1.024221e+06   \n",
    "\n",
    "                            Sum Sq.res    F value    Pr(>F)  \n",
    "prompt                    1.896052e+07   4.428664  0.000012  \n",
    "fine_tune_dataset         1.956659e+07  19.038352  0.000015  \n",
    "prompt:fine_tune_dataset  1.916384e+07   3.586788  0.000231  \n",
    "\"\"\"\n",
    "\n",
    "liberty_text = \"\"\"\n",
    "-------------------- Gemma --------------------\n",
    "Levene’s Test: Statistic = 9.7240, p-value = 1.791e-25\n",
    "Robust ANOVA\n",
    "                                              Term   Df  Df.res        Sum Sq  \\\n",
    "prompt                                      prompt  9.0   700.0  8.734504e+06   \n",
    "fine_tune_dataset                fine_tune_dataset  1.0   700.0  2.813587e+05   \n",
    "prompt:fine_tune_dataset  prompt:fine_tune_dataset  9.0   700.0  1.000707e+06   \n",
    "\n",
    "                            Sum Sq.res    F value        Pr(>F)  \n",
    "prompt                    2.232694e+07  30.427377  5.542028e-45  \n",
    "fine_tune_dataset         3.071099e+07   6.413050  1.154592e-02  \n",
    "prompt:fine_tune_dataset  3.006898e+07   2.588473  6.126808e-03  \n",
    "-------------------- Llama --------------------\n",
    "Levene’s Test: Statistic = 3.7303, p-value = 1.743e-07\n",
    "Robust ANOVA\n",
    "                                              Term   Df  Df.res        Sum Sq  \\\n",
    "prompt                                      prompt  9.0   700.0  7.396102e+05   \n",
    "fine_tune_dataset                fine_tune_dataset  1.0   700.0  6.569610e+05   \n",
    "prompt:fine_tune_dataset  prompt:fine_tune_dataset  9.0   700.0  1.053423e+06   \n",
    "\n",
    "                            Sum Sq.res    F value    Pr(>F)  \n",
    "prompt                    3.034109e+07   1.895952  0.049617  \n",
    "fine_tune_dataset         3.039991e+07  15.127435  0.000110  \n",
    "prompt:fine_tune_dataset  3.000626e+07   2.730528  0.003889  \n",
    "-------------------- Falcon --------------------\n",
    "Levene’s Test: Statistic = 8.7122, p-value = 3.931e-22\n",
    "Robust ANOVA\n",
    "                                              Term   Df  Df.res        Sum Sq  \\\n",
    "prompt                                      prompt  9.0   620.0  1.795760e+06   \n",
    "fine_tune_dataset                fine_tune_dataset  1.0   620.0  1.883358e+06   \n",
    "prompt:fine_tune_dataset  prompt:fine_tune_dataset  9.0   620.0  4.549089e+05   \n",
    "\n",
    "                            Sum Sq.res    F value        Pr(>F)  \n",
    "prompt                    1.988920e+07   6.219852  1.977477e-08  \n",
    "fine_tune_dataset         1.968142e+07  59.329170  5.302777e-14  \n",
    "prompt:fine_tune_dataset  2.110280e+07   1.485024  1.495958e-01  \n",
    "-------------------- Mistral --------------------\n",
    "Levene’s Test: Statistic = 12.0614, p-value = 1.384e-31\n",
    "Robust ANOVA\n",
    "                                              Term   Df  Df.res        Sum Sq  \\\n",
    "prompt                                      prompt  9.0   604.0  1.076232e+06   \n",
    "fine_tune_dataset                fine_tune_dataset  1.0   604.0  7.366785e+05   \n",
    "prompt:fine_tune_dataset  prompt:fine_tune_dataset  9.0   604.0  8.167706e+05   \n",
    "\n",
    "                            Sum Sq.res    F value    Pr(>F)  \n",
    "prompt                    1.906839e+07   3.787795  0.000116  \n",
    "fine_tune_dataset         1.949502e+07  22.823976  0.000002  \n",
    "prompt:fine_tune_dataset  1.939441e+07   2.826298  0.002906\n",
    "\"\"\"\n",
    "\n",
    "tradition_text = \"\"\"\n",
    "-------------------- Gemma --------------------\n",
    "Levene’s Test: Statistic = 7.7240, p-value = 1.774e-19\n",
    "Robust ANOVA\n",
    "                                              Term   Df  Df.res        Sum Sq  \\\n",
    "prompt                                      prompt  9.0   700.0  1.109152e+07   \n",
    "fine_tune_dataset                fine_tune_dataset  1.0   700.0  1.965371e+06   \n",
    "prompt:fine_tune_dataset  prompt:fine_tune_dataset  9.0   700.0  2.470409e+06   \n",
    "\n",
    "                            Sum Sq.res    F value        Pr(>F)  \n",
    "prompt                    1.997010e+07  43.198284  1.397884e-61  \n",
    "fine_tune_dataset         2.907134e+07  47.323560  1.336644e-11  \n",
    "prompt:fine_tune_dataset  2.854520e+07   6.731182  2.736331e-09  \n",
    "-------------------- Llama --------------------\n",
    "Levene’s Test: Statistic = 4.9623, p-value = 4.046e-11\n",
    "Robust ANOVA\n",
    "                                              Term   Df  Df.res  \\\n",
    "prompt                                      prompt  9.0   700.0   \n",
    "fine_tune_dataset                fine_tune_dataset  1.0   700.0   \n",
    "prompt:fine_tune_dataset  prompt:fine_tune_dataset  9.0   700.0   \n",
    "\n",
    "                                 Sum Sq    Sum Sq.res    F value    Pr(>F)  \n",
    "prompt                    412082.145833  3.064986e+07   1.045709  0.401613  \n",
    "fine_tune_dataset         588123.126563  3.046684e+07  13.512599  0.000255  \n",
    "prompt:fine_tune_dataset  688067.018924  3.033484e+07   1.764187  0.071653  \n",
    "-------------------- Falcon --------------------\n",
    "Levene’s Test: Statistic = 13.5853, p-value = 6.384e-36\n",
    "Robust ANOVA\n",
    "                                              Term   Df  Df.res        Sum Sq  \\\n",
    "prompt                                      prompt  9.0   620.0  2.155445e+06   \n",
    "fine_tune_dataset                fine_tune_dataset  1.0   620.0  3.551917e+05   \n",
    "prompt:fine_tune_dataset  prompt:fine_tune_dataset  9.0   620.0  2.525440e+05   \n",
    "\n",
    "                            Sum Sq.res    F value        Pr(>F)  \n",
    "prompt                    1.957793e+07   7.584369  1.368145e-10  \n",
    "fine_tune_dataset         2.134329e+07  10.317946  1.385652e-03  \n",
    "prompt:fine_tune_dataset  2.143501e+07   0.811638  6.056107e-01  \n",
    "-------------------- Mistral --------------------\n",
    "Levene’s Test: Statistic = 4.7932, p-value = 1.681e-10\n",
    "Robust ANOVA\n",
    "                                              Term   Df  Df.res        Sum Sq  \\\n",
    "prompt                                      prompt  9.0   604.0  2.643219e+06   \n",
    "fine_tune_dataset                fine_tune_dataset  1.0   604.0  1.049732e+02   \n",
    "prompt:fine_tune_dataset  prompt:fine_tune_dataset  9.0   604.0  5.424615e+05   \n",
    "\n",
    "                            Sum Sq.res    F value        Pr(>F)  \n",
    "prompt                    1.755333e+07  10.105741  1.483741e-14  \n",
    "fine_tune_dataset         2.018733e+07   0.003141  9.553264e-01  \n",
    "prompt:fine_tune_dataset  1.962662e+07   1.854889  5.602415e-02  \n",
    "\"\"\"\n",
    "\n",
    "def return_dependent_var_df(text, dependent_var):\n",
    "    blocks = re.split(r'-{10,} (.+?) -{10,}', text)[1:]\n",
    "    \n",
    "    # Prepare CSV structure\n",
    "    results = []\n",
    "    \n",
    "    for i in range(0, len(blocks), 2):\n",
    "        model = blocks[i].strip()\n",
    "        content = blocks[i + 1].strip()\n",
    "    \n",
    "        # Find lines containing the ANOVA results\n",
    "        lines = content.strip().split('\\n')\n",
    "        anova_lines = [\n",
    "            line.strip()\n",
    "            for line in lines\n",
    "            if line.strip().startswith(\"prompt\") or\n",
    "               line.strip().startswith(\"fine_tune_dataset\") or\n",
    "               line.strip().startswith(\"prompt:fine_tune_dataset\")\n",
    "        ]\n",
    "    \n",
    "        # Dictionary to hold the output row\n",
    "        row = {\"model\": model}\n",
    "    \n",
    "        for line in anova_lines:\n",
    "            parts = re.split(r'\\s{2,}', line.strip())\n",
    "            if len(parts) < 3:\n",
    "                continue  # Not enough fields\n",
    "    \n",
    "            term = parts[0]\n",
    "            f_value = parts[-2]\n",
    "            p_value = parts[-1]\n",
    "    \n",
    "            if term == \"prompt\":\n",
    "                row[f\"{dependent_var}-prompt-f-score\"] = f\"{float(f_value):.2e}\"\n",
    "                row[f\"{dependent_var}-prompt-p-value\"] = f\"{float(p_value):.2e}\"\n",
    "            elif term == \"fine_tune_dataset\":\n",
    "                row[f\"{dependent_var}-finetune-f-score\"] = f\"{float(f_value):.2e}\"\n",
    "                row[f\"{dependent_var}-finetune-p-value\"] = f\"{float(p_value):.2e}\"\n",
    "            elif term == \"prompt:fine_tune_dataset\":\n",
    "                row[f\"{dependent_var}-prompt-finetune-interaction-f-score\"] = f\"{float(f_value):.2e}\"\n",
    "                row[f\"{dependent_var}-prompt-finetune-interaction-p-value\"] = f\"{float(p_value):.2e}\"\n",
    "        results.append(row)\n",
    "\n",
    "    return pd.DataFrame(results, columns=[\"model\", f\"{dependent_var}-prompt-f-score\",\n",
    "                                          f\"{dependent_var}-prompt-p-value\",\n",
    "                                         f\"{dependent_var}-finetune-f-score\",\n",
    "                                         f\"{dependent_var}-finetune-p-value\",\n",
    "                                         f\"{dependent_var}-prompt-finetune-interaction-f-score\",\n",
    "                                         f\"{dependent_var}-prompt-finetune-interaction-p-value\"])\n",
    "\n",
    "df_equality = return_dependent_var_df(equality_text, dependent_var=\"equality\")\n",
    "df_nation = return_dependent_var_df(nation_text, dependent_var=\"nation\")\n",
    "df_liberty = return_dependent_var_df(liberty_text, dependent_var=\"liberty\")\n",
    "df_tradition = return_dependent_var_df(tradition_text, dependent_var=\"tradition\")\n",
    "stacked = pd.concat([df_equality, df_nation, df_liberty, df_tradition], axis=1)\n",
    "stacked.to_csv(\"multi_way_anova_results_8_values.csv\", index=False)\n",
    "print(f\"output written\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4eefaa-de0f-4c5c-8a76-478d8fd23adb",
   "metadata": {},
   "source": [
    "### Does the type of fine-tuning dataset make a difference?\n",
    "\n",
    "Check if the type of the fine-tuning dataset makes a difference. We need to re-download the data because we have converted all fine-tuning datasets to \"fine_tuned\" in the last step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b040a124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2704\n",
      "{'Newsroom', 'Pol-Convo', 'Imdb', 'FineTome', 'Newsarticles', 'Base', 'Canadian-QA', 'OpenR1', 'Scisumm'}\n",
      "2704\n",
      "Mistral 624\n",
      "Falcon 640\n",
      "Gemma 720\n",
      "Llama 720\n",
      "{'Mistral', 'Falcon', 'Gemma', 'Llama'}\n"
     ]
    }
   ],
   "source": [
    "url = \"https://docs.google.com/spreadsheets/d/1Opmxs3CoUepJTnPSQSwkztpiMnpda5RO7dMt1p-uSjc/edit?gid=0#gid=0\"\n",
    "df = read_gsheet(url=url)\n",
    "print(len(df))\n",
    "df = df.drop_duplicates()\n",
    "df = df[df.model != \"Phi\"]  # filter out Phi\n",
    "print(set(df['fine_tune_dataset']))\n",
    "df['prompt'] = df['prompt'].astype(str)\n",
    "print(len(df))\n",
    "\n",
    "\n",
    "model_split_df = {}\n",
    "\n",
    "columns = {}\n",
    "columns['equality'] = \"Equality\"\n",
    "columns['nation'] = \"Nation\"\n",
    "columns['liberty'] = \"Liberty\"\n",
    "columns['tradition'] = \"Tradition\"\n",
    "\n",
    "\n",
    "for model in set(df['model']):\n",
    "    _df = df[(df['model']== model)]\n",
    "    _df.rename(columns={v:k for k,v in columns.items()}, inplace=True)\n",
    "    print(model, len(_df))\n",
    "    model_split_df[model] = _df\n",
    "\n",
    "print(set(df['model']))\n",
    "models = [\"Gemma\", \"Llama\", \"Falcon\", \"Mistral\"]\n",
    "dependent_vars = [\"equality\", \"nation\", \"liberty\", \"tradition\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b4f1e62f-9964-4c0c-8f62-28e7e4db695b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=: equality =:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:\n",
      "-------------------- Gemma --------------------\n",
      "Multiple Comparison of Means - Tukey HSD, FWER=0.05\n",
      "===================================================\n",
      " group1 group2 meandiff p-adj  lower  upper  reject\n",
      "---------------------------------------------------\n",
      "control target  -2.0525 0.0001 -3.078 -1.027   True\n",
      "---------------------------------------------------\n",
      "-------------------- Llama --------------------\n",
      "Multiple Comparison of Means - Tukey HSD, FWER=0.05 \n",
      "====================================================\n",
      " group1 group2 meandiff p-adj   lower  upper  reject\n",
      "----------------------------------------------------\n",
      "control target    0.995 0.0796 -0.1177 2.1077  False\n",
      "----------------------------------------------------\n",
      "-------------------- Falcon --------------------\n",
      "Multiple Comparison of Means - Tukey HSD, FWER=0.05 \n",
      "====================================================\n",
      " group1 group2 meandiff p-adj   lower  upper  reject\n",
      "----------------------------------------------------\n",
      "control target   0.4762 0.2609 -0.3549 1.3074  False\n",
      "----------------------------------------------------\n",
      "-------------------- Mistral --------------------\n",
      "Multiple Comparison of Means - Tukey HSD, FWER=0.05\n",
      "==================================================\n",
      " group1 group2 meandiff p-adj lower  upper  reject\n",
      "--------------------------------------------------\n",
      "control target   6.3447   0.0 4.8837 7.8058   True\n",
      "--------------------------------------------------\n",
      "=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=: nation =:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:\n",
      "-------------------- Gemma --------------------\n",
      "Multiple Comparison of Means - Tukey HSD, FWER=0.05\n",
      "=================================================\n",
      " group1 group2 meandiff p-adj lower  upper reject\n",
      "-------------------------------------------------\n",
      "control target   3.7587   0.0 2.9005 4.617   True\n",
      "-------------------------------------------------\n",
      "-------------------- Llama --------------------\n",
      "Multiple Comparison of Means - Tukey HSD, FWER=0.05 \n",
      "====================================================\n",
      " group1 group2 meandiff p-adj   lower  upper  reject\n",
      "----------------------------------------------------\n",
      "control target   0.9637 0.0637 -0.0549 1.9824  False\n",
      "----------------------------------------------------\n",
      "-------------------- Falcon --------------------\n",
      "Multiple Comparison of Means - Tukey HSD, FWER=0.05\n",
      "===================================================\n",
      " group1 group2 meandiff p-adj  lower  upper  reject\n",
      "---------------------------------------------------\n",
      "control target   0.3971 0.2581 -0.292 1.0863  False\n",
      "---------------------------------------------------\n",
      "-------------------- Mistral --------------------\n",
      "Multiple Comparison of Means - Tukey HSD, FWER=0.05 \n",
      "====================================================\n",
      " group1 group2 meandiff p-adj  lower   upper  reject\n",
      "----------------------------------------------------\n",
      "control target  -5.4726   0.0 -6.4694 -4.4758   True\n",
      "----------------------------------------------------\n",
      "=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=: liberty =:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:\n",
      "-------------------- Gemma --------------------\n",
      "Multiple Comparison of Means - Tukey HSD, FWER=0.05\n",
      "===================================================\n",
      " group1 group2 meandiff p-adj  lower  upper  reject\n",
      "---------------------------------------------------\n",
      "control target  -4.7716   0.0 -6.0761 -3.467   True\n",
      "---------------------------------------------------\n",
      "-------------------- Llama --------------------\n",
      "Multiple Comparison of Means - Tukey HSD, FWER=0.05\n",
      "===================================================\n",
      " group1 group2 meandiff p-adj  lower  upper  reject\n",
      "---------------------------------------------------\n",
      "control target   0.9244 0.054 -0.0161 1.8648  False\n",
      "---------------------------------------------------\n",
      "-------------------- Falcon --------------------\n",
      "Multiple Comparison of Means - Tukey HSD, FWER=0.05\n",
      "===================================================\n",
      " group1 group2 meandiff p-adj  lower  upper  reject\n",
      "---------------------------------------------------\n",
      "control target   1.0542 0.0196 0.1695 1.9388   True\n",
      "---------------------------------------------------\n",
      "-------------------- Mistral --------------------\n",
      "Multiple Comparison of Means - Tukey HSD, FWER=0.05\n",
      "==================================================\n",
      " group1 group2 meandiff p-adj lower  upper  reject\n",
      "--------------------------------------------------\n",
      "control target   2.8085   0.0 1.7932 3.8239   True\n",
      "--------------------------------------------------\n",
      "=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=: tradition =:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:\n",
      "-------------------- Gemma --------------------\n",
      "Multiple Comparison of Means - Tukey HSD, FWER=0.05\n",
      "===================================================\n",
      " group1 group2 meandiff p-adj  lower  upper  reject\n",
      "---------------------------------------------------\n",
      "control target   1.5391 0.0025 0.5428 2.5353   True\n",
      "---------------------------------------------------\n",
      "-------------------- Llama --------------------\n",
      "Multiple Comparison of Means - Tukey HSD, FWER=0.05 \n",
      "====================================================\n",
      " group1 group2 meandiff p-adj   lower  upper  reject\n",
      "----------------------------------------------------\n",
      "control target  -1.5844 0.0002 -2.4168 -0.752   True\n",
      "----------------------------------------------------\n",
      "-------------------- Falcon --------------------\n",
      "Multiple Comparison of Means - Tukey HSD, FWER=0.05 \n",
      "====================================================\n",
      " group1 group2 meandiff p-adj  lower   upper  reject\n",
      "----------------------------------------------------\n",
      "control target  -2.4154   0.0 -3.3006 -1.5301   True\n",
      "----------------------------------------------------\n",
      "-------------------- Mistral --------------------\n",
      "Multiple Comparison of Means - Tukey HSD, FWER=0.05 \n",
      "====================================================\n",
      " group1 group2 meandiff p-adj  lower   upper  reject\n",
      "----------------------------------------------------\n",
      "control target  -8.2771   0.0 -9.2191 -7.3352   True\n",
      "----------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "dataset_category = {\"target\": {'Canadian-QA', 'Newsarticles', 'Newsroom', 'Pol-Convo'}, \"control\": {\"Imdb\", \"OpenR1\", \"Scisumm\", \"FineTome\"}}\n",
    "# we will test the fuck all \n",
    "def tukey_test(_df, _dependent_var='output_x'):\n",
    "    _df = _df[_df['fine_tune_dataset'] != 'Base']\n",
    "    #print(len(_df), list(_df), set(_df['fine_tune_dataset']))\n",
    "    _df['dataset_group'] = _df['fine_tune_dataset'].apply(lambda x: 'target' if x in dataset_category[\"target\"] else \"control\")\n",
    "    #model = ols(f'{_dependent_var} ~ C(prompt) + C(fine_tune_dataset) + C(prompt):C(fine_tune_dataset)', data=_df).fit()\n",
    "    #anova_table = sm.stats.anova_lm(model, typ=2)\n",
    "    tukey = pairwise_tukeyhsd(endog=_df[_dependent_var],\n",
    "                              groups=_df[\"dataset_group\"],\n",
    "                              alpha=0.05)\n",
    "    return tukey\n",
    "\n",
    "for dependent_var in dependent_vars:\n",
    "    print(\"=:\"*20+f\" {dependent_var} \"+\"=:\"*20)\n",
    "    for model in models:\n",
    "        print(\"-\"*20+f\" {model} \"+\"-\"*20)\n",
    "        print(tukey_test(_df=model_split_df[model], _dependent_var = dependent_var))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "109e42b5-8a0b-471c-97b7-ade132a8e1fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=: equality =:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:\n",
      "-------------------- Gemma --------------------\n",
      "Task: classification\n",
      "       diff      pval\n",
      "0  4.06e+00  6.49e-10\n",
      "1  8.93e+00  0.00e+00\n",
      "2  4.86e+00  2.91e-09\n",
      "Task: summarization\n",
      "        diff      pval\n",
      "0   1.11e+00  2.35e-01\n",
      "1  -1.17e-01  9.86e-01\n",
      "2  -1.22e+00  3.21e-01\n",
      "Task: conversational\n",
      "        diff      pval\n",
      "0  -2.15e+00  4.40e-04\n",
      "1  -5.53e+00  0.00e+00\n",
      "2  -3.37e+00  4.63e-08\n",
      "Task: qa\n",
      "        diff      pval\n",
      "0  -8.77e-01  2.86e-01\n",
      "1   7.06e+00  4.07e-12\n",
      "2   7.94e+00  1.70e-14\n",
      "-------------------- Llama --------------------\n",
      "Task: classification\n",
      "       diff      pval\n",
      "0  4.07e+00  3.56e-07\n",
      "1  1.22e+01  0.00e+00\n",
      "2  8.11e+00  3.02e-14\n",
      "Task: summarization\n",
      "       diff      pval\n",
      "0  1.70e+00  3.83e-02\n",
      "1  3.85e+00  3.55e-04\n",
      "2  2.15e+00  6.13e-02\n",
      "Task: conversational\n",
      "        diff      pval\n",
      "0   1.05e+01  8.44e-15\n",
      "1   3.15e+00  1.77e-04\n",
      "2  -7.33e+00  2.32e-13\n",
      "Task: qa\n",
      "        diff      pval\n",
      "0   7.51e+00  2.23e-09\n",
      "1   6.03e-01  7.11e-01\n",
      "2  -6.91e+00  4.76e-08\n",
      "-------------------- Falcon --------------------\n",
      "Task: classification\n",
      "        diff      pval\n",
      "0   4.17e+00  2.21e-07\n",
      "1   3.47e+00  7.52e-07\n",
      "2  -7.00e-01  4.37e-01\n",
      "Task: summarization\n",
      "       diff      pval\n",
      "0  1.64e+00  4.92e-02\n",
      "1  2.29e+00  2.38e-03\n",
      "2  6.45e-01  4.71e-01\n",
      "Task: conversational\n",
      "        diff      pval\n",
      "0  -6.37e-01  6.79e-01\n",
      "1  -6.68e+00  3.76e-11\n",
      "2  -6.04e+00  6.24e-10\n",
      "Task: qa\n",
      "       diff      pval\n",
      "0  6.00e-02  9.98e-01\n",
      "1  2.54e+00  3.85e-03\n",
      "2  2.48e+00  2.69e-02\n",
      "-------------------- Mistral --------------------\n",
      "Task: classification\n",
      "       diff      pval\n",
      "0  5.15e-01  6.60e-01\n",
      "1  1.06e+01  2.66e-14\n",
      "2  1.01e+01  0.00e+00\n",
      "Task: summarization\n",
      "        diff      pval\n",
      "0   2.14e+01  7.77e-16\n",
      "1   1.06e+01  0.00e+00\n",
      "2  -1.08e+01  3.06e-14\n",
      "Task: conversational\n",
      "        diff      pval\n",
      "0   1.78e+01  0.00e+00\n",
      "1   2.98e+00  2.20e-03\n",
      "2  -1.48e+01  2.55e-14\n",
      "Task: qa\n",
      "        diff      pval\n",
      "0   2.06e+01  0.00e+00\n",
      "1   4.69e+00  5.16e-07\n",
      "2  -1.59e+01  0.00e+00\n",
      "=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=: nation =:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:\n",
      "-------------------- Gemma --------------------\n",
      "Task: classification\n",
      "        diff      pval\n",
      "0   7.12e-02  9.95e-01\n",
      "1  -6.11e+00  3.79e-13\n",
      "2  -6.18e+00  5.93e-11\n",
      "Task: summarization\n",
      "        diff      pval\n",
      "0  -2.05e+00  9.88e-04\n",
      "1  -6.52e+00  0.00e+00\n",
      "2  -4.47e+00  0.00e+00\n",
      "Task: conversational\n",
      "        diff      pval\n",
      "0  -5.50e+00  8.69e-11\n",
      "1  -6.05e-01  5.94e-01\n",
      "2   4.90e+00  9.80e-10\n",
      "Task: qa\n",
      "        diff      pval\n",
      "0  -2.64e+00  5.74e-03\n",
      "1  -1.19e+01  0.00e+00\n",
      "2  -9.28e+00  0.00e+00\n",
      "-------------------- Llama --------------------\n",
      "Task: classification\n",
      "        diff      pval\n",
      "0   1.29e-01  9.87e-01\n",
      "1  -7.12e+00  0.00e+00\n",
      "2  -7.24e+00  2.61e-14\n",
      "Task: summarization\n",
      "        diff      pval\n",
      "0  -2.06e+00  1.27e-02\n",
      "1  -8.23e+00  1.75e-14\n",
      "2  -6.17e+00  4.47e-11\n",
      "Task: conversational\n",
      "        diff      pval\n",
      "0  -1.13e+01  9.10e-15\n",
      "1  -2.57e+00  1.55e-03\n",
      "2   8.70e+00  0.00e+00\n",
      "Task: qa\n",
      "        diff      pval\n",
      "0  -8.96e+00  0.00e+00\n",
      "1  -8.11e+00  0.00e+00\n",
      "2   8.59e-01  6.70e-01\n",
      "-------------------- Falcon --------------------\n",
      "Task: classification\n",
      "       diff      pval\n",
      "0  1.31e+00  4.69e-02\n",
      "1  1.33e+00  8.53e-03\n",
      "2  2.00e-02  9.99e-01\n",
      "Task: summarization\n",
      "       diff      pval\n",
      "0  7.10e-01  2.24e-01\n",
      "1  8.95e-01  1.86e-01\n",
      "2  1.85e-01  9.34e-01\n",
      "Task: conversational\n",
      "       diff      pval\n",
      "0  4.24e+00  1.14e-06\n",
      "1  4.38e+00  2.01e-08\n",
      "2  1.43e-01  9.87e-01\n",
      "Task: qa\n",
      "        diff      pval\n",
      "0   2.56e+00  1.02e-04\n",
      "1   1.17e+00  1.39e-02\n",
      "2  -1.39e+00  5.07e-02\n",
      "-------------------- Mistral --------------------\n",
      "Task: classification\n",
      "        diff      pval\n",
      "0   2.32e+00  6.95e-04\n",
      "1  -2.30e+00  6.81e-05\n",
      "2  -4.62e+00  8.77e-15\n",
      "Task: summarization\n",
      "        diff      pval\n",
      "0  -1.13e+01  0.00e+00\n",
      "1  -3.88e+00  3.22e-09\n",
      "2   7.39e+00  0.00e+00\n",
      "Task: conversational\n",
      "        diff      pval\n",
      "0  -9.24e+00  0.00e+00\n",
      "1  -4.80e-01  7.87e-01\n",
      "2   8.76e+00  0.00e+00\n",
      "Task: qa\n",
      "        diff      pval\n",
      "0  -9.23e+00  0.00e+00\n",
      "1   3.29e+00  3.26e-06\n",
      "2   1.25e+01  0.00e+00\n",
      "=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=: liberty =:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:\n",
      "-------------------- Gemma --------------------\n",
      "Task: classification\n",
      "        diff      pval\n",
      "0  -2.55e+00  9.19e-03\n",
      "1   7.21e+00  4.22e-15\n",
      "2   9.76e+00  0.00e+00\n",
      "Task: summarization\n",
      "        diff      pval\n",
      "0  -1.83e+00  8.79e-02\n",
      "1   8.12e+00  2.49e-14\n",
      "2   9.96e+00  2.69e-14\n",
      "Task: conversational\n",
      "        diff      pval\n",
      "0   9.28e+00  0.00e+00\n",
      "1   6.90e+00  7.38e-11\n",
      "2  -2.38e+00  8.10e-02\n",
      "Task: qa\n",
      "        diff      pval\n",
      "0  -5.19e+00  1.76e-06\n",
      "1  -3.44e+00  5.57e-04\n",
      "2   1.75e+00  2.80e-01\n",
      "-------------------- Llama --------------------\n",
      "Task: classification\n",
      "        diff      pval\n",
      "0  -1.06e+00  1.83e-01\n",
      "1   5.61e+00  4.22e-13\n",
      "2   6.67e+00  0.00e+00\n",
      "Task: summarization\n",
      "        diff      pval\n",
      "0  -5.49e-01  6.10e-01\n",
      "1   3.69e+00  2.37e-07\n",
      "2   4.23e+00  1.26e-09\n",
      "Task: conversational\n",
      "        diff      pval\n",
      "0   5.11e+00  3.92e-14\n",
      "1  -4.07e+00  1.48e-06\n",
      "2  -9.19e+00  0.00e+00\n",
      "Task: qa\n",
      "        diff      pval\n",
      "0   7.56e+00  3.99e-14\n",
      "1   2.14e+00  6.95e-03\n",
      "2  -5.42e+00  1.55e-07\n",
      "-------------------- Falcon --------------------\n",
      "Task: classification\n",
      "        diff      pval\n",
      "0  -3.58e+00  1.48e-09\n",
      "1  -2.99e+00  1.40e-09\n",
      "2   5.95e-01  6.00e-01\n",
      "Task: summarization\n",
      "        diff      pval\n",
      "0  -1.55e+00  1.88e-04\n",
      "1  -2.24e+00  1.05e-05\n",
      "2  -6.95e-01  3.68e-01\n",
      "Task: conversational\n",
      "        diff      pval\n",
      "0  -3.36e+00  1.01e-04\n",
      "1  -1.13e+01  1.91e-14\n",
      "2  -7.97e+00  3.03e-08\n",
      "Task: qa\n",
      "        diff      pval\n",
      "0  -6.80e+00  6.66e-15\n",
      "1  -3.56e+00  2.52e-07\n",
      "2   3.24e+00  4.58e-05\n",
      "-------------------- Mistral --------------------\n",
      "Task: classification\n",
      "        diff      pval\n",
      "0  -4.49e+00  3.64e-14\n",
      "1   6.51e+00  8.88e-15\n",
      "2   1.10e+01  0.00e+00\n",
      "Task: summarization\n",
      "        diff      pval\n",
      "0   1.00e+01  0.00e+00\n",
      "1   5.60e+00  1.55e-15\n",
      "2  -4.43e+00  5.11e-15\n",
      "Task: conversational\n",
      "        diff      pval\n",
      "0   9.06e+00  3.73e-14\n",
      "1   1.06e+00  1.79e-01\n",
      "2  -8.01e+00  0.00e+00\n",
      "Task: qa\n",
      "        diff      pval\n",
      "0   1.28e+01  0.00e+00\n",
      "1  -3.55e+00  1.01e-08\n",
      "2  -1.64e+01  0.00e+00\n",
      "=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=: tradition =:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:\n",
      "-------------------- Gemma --------------------\n",
      "Task: classification\n",
      "        diff      pval\n",
      "0  -9.59e-01  3.82e-01\n",
      "1  -3.58e+00  3.33e-05\n",
      "2  -2.62e+00  1.35e-02\n",
      "Task: summarization\n",
      "        diff      pval\n",
      "0   1.86e-01  9.75e-01\n",
      "1  -6.20e+00  6.26e-11\n",
      "2  -6.39e+00  4.84e-08\n",
      "Task: conversational\n",
      "        diff      pval\n",
      "0  -8.04e+00  0.00e+00\n",
      "1  -5.53e+00  3.00e-13\n",
      "2   2.51e+00  9.25e-03\n",
      "Task: qa\n",
      "        diff      pval\n",
      "0  -1.37e+00  1.86e-01\n",
      "1  -1.03e+00  2.48e-01\n",
      "2   3.39e-01  9.16e-01\n",
      "-------------------- Llama --------------------\n",
      "Task: classification\n",
      "        diff      pval\n",
      "0   2.98e+00  5.28e-10\n",
      "1   9.21e-01  3.23e-01\n",
      "2  -2.06e+00  1.13e-02\n",
      "Task: summarization\n",
      "        diff      pval\n",
      "0   1.01e+00  3.03e-03\n",
      "1   1.15e-01  9.69e-01\n",
      "2  -8.96e-01  1.47e-01\n",
      "Task: conversational\n",
      "        diff      pval\n",
      "0  -3.52e+00  2.19e-11\n",
      "1   3.04e+00  3.65e-06\n",
      "2   6.57e+00  4.88e-15\n",
      "Task: qa\n",
      "       diff      pval\n",
      "0  2.17e+00  5.30e-02\n",
      "1  4.90e+00  4.30e-10\n",
      "2  2.73e+00  3.86e-02\n",
      "-------------------- Falcon --------------------\n",
      "Task: classification\n",
      "        diff      pval\n",
      "0  -6.55e-01  5.14e-01\n",
      "1   2.75e-01  8.16e-01\n",
      "2   9.30e-01  2.81e-01\n",
      "Task: summarization\n",
      "       diff      pval\n",
      "0  1.70e-01  9.33e-01\n",
      "1  1.99e+00  6.23e-03\n",
      "2  1.82e+00  1.99e-02\n",
      "Task: conversational\n",
      "       diff      pval\n",
      "0  2.92e+00  1.66e-03\n",
      "1  1.13e+01  3.39e-14\n",
      "2  8.36e+00  0.00e+00\n",
      "Task: qa\n",
      "       diff      pval\n",
      "0  1.19e+00  6.53e-02\n",
      "1  2.15e+00  3.36e-03\n",
      "2  9.62e-01  3.68e-01\n",
      "-------------------- Mistral --------------------\n",
      "Task: classification\n",
      "       diff      pval\n",
      "0  2.00e-02  9.99e-01\n",
      "1  1.65e-01  9.28e-01\n",
      "2  1.45e-01  9.41e-01\n",
      "Task: summarization\n",
      "        diff      pval\n",
      "0  -7.76e+00  1.72e-14\n",
      "1  -1.02e-01  9.90e-01\n",
      "2   7.65e+00  2.78e-15\n",
      "Task: conversational\n",
      "        diff      pval\n",
      "0  -5.11e+00  9.71e-12\n",
      "1   3.79e+00  1.66e-13\n",
      "2   8.89e+00  0.00e+00\n",
      "Task: qa\n",
      "        diff      pval\n",
      "0  -1.69e+01  0.00e+00\n",
      "1   1.03e+01  0.00e+00\n",
      "2   2.72e+01  0.00e+00\n",
      "output written\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pingouin as pg\n",
    "from statsmodels.formula.api import ols\n",
    "import statsmodels.api as sm\n",
    "\n",
    "dataset_category = {\n",
    "    \"target\": {'Canadian-QA', 'Newsarticles', 'Newsroom', 'Pol-Convo'},\n",
    "    \"control\": {\"Imdb\", \"OpenR1\", \"Scisumm\", \"FineTome\"}\n",
    "}\n",
    "\n",
    "task_dataset_category = {\n",
    "    \"classification\": {\n",
    "        \"target\": 'Newsarticles',\n",
    "        \"control\": \"Imdb\"\n",
    "    },\n",
    "    \"summarization\": {\n",
    "        \"target\": \"Newsroom\",\n",
    "        \"control\": \"Scisumm\"\n",
    "    },\n",
    "    \"conversational\": {\n",
    "        \"target\": \"Pol-Convo\",\n",
    "        \"control\": \"FineTome\"\n",
    "    },\n",
    "    \"qa\": {\n",
    "        \"target\": \"Canadian-QA\",\n",
    "        \"control\": \"OpenR1\"\n",
    "    }\n",
    "}\n",
    "\n",
    "def games_howell_test_base(_df, _dependent_var='output_x'):\n",
    "    _df['dataset_group'] = _df['fine_tune_dataset'].apply(\n",
    "        lambda x: 'Base' if x == 'Base' else ('target' if x in dataset_category[\"target\"] else \"control\")\n",
    "    )\n",
    "    assert set(_df['dataset_group']) == {'target', 'control', 'Base'}, \"Dataset groups must be either 'target' or 'control'\"\n",
    "    gh_result = pg.pairwise_gameshowell(dv=_dependent_var, between=\"dataset_group\", data=_df)\n",
    "    return gh_result\n",
    "    \n",
    "def games_howell_test_task_base(_df, task, _dependent_var='output_x'):\n",
    "    task_datasets = set(task_dataset_category[task].values()) | {'Base'}\n",
    "    _df = _df[_df['fine_tune_dataset'].isin(task_datasets)]\n",
    "    if _df['fine_tune_dataset'].nunique() < 3:\n",
    "        print(f\"Only {_df['fine_tune_dataset'].nunique()} datasets found for task '{task}', returning empty DataFrame.\")\n",
    "        return None\n",
    "    _df['dataset_group'] = _df['fine_tune_dataset'].apply(\n",
    "        lambda x: 'Base' if x == 'Base' else ('target' if x in task_dataset_category[task][\"target\"] else \"control\")\n",
    "    )\n",
    "    assert set(_df['dataset_group']) == {'target', 'control', 'Base'}, f\"Dataset groups must be either 'target' or 'control' or 'Base', got {set(_df['dataset_group'])}\"\n",
    "    gh_result = pg.pairwise_gameshowell(dv=_dependent_var, between=\"dataset_group\", data=_df)\n",
    "    return gh_result\n",
    "\n",
    "\n",
    "# dfs = []\n",
    "# for dependent_var in dependent_vars:\n",
    "#     print(\"=:\" * 20 + f\" {dependent_var} \" + \"=:\" * 20)\n",
    "#     for model in models:\n",
    "#         print(\"-\" * 20 + f\" {model} \" + \"-\" * 20)\n",
    "#         result = games_howell_test_base(_df=model_split_df[model], _dependent_var=dependent_var)\n",
    "#         result['model'] = model\n",
    "#         result['dependent_var'] = dependent_var\n",
    "#         result['setup'] = [f\"{x.lower()}-{y.lower()}\" for x,y in zip(result['A'], result['B'])]\n",
    "#         result['diff'] = [f\"{float(x):.2e}\" for x in result['diff']]\n",
    "#         result['pval'] = [f\"{float(x):.2e}\" for x in result['pval']]\n",
    "#         #print(result[['diff', 'pval']])\n",
    "#         dfs.append(result[['model', 'setup', 'diff', 'pval', 'dependent_var']])\n",
    "\n",
    "# pd.concat(dfs, axis=0).to_csv(\"diff_pval_summary_incl_base.csv\", index=False)\n",
    "# print(\"output written\")\n",
    "\n",
    "dfs = []\n",
    "for dependent_var in dependent_vars:\n",
    "    print(\"=:\" * 20 + f\" {dependent_var} \" + \"=:\" * 20)\n",
    "    for model in models:\n",
    "        print(\"-\" * 20 + f\" {model} \" + \"-\" * 20)\n",
    "        for task in task_dataset_category.keys():\n",
    "            print(f\"Task: {task}\")\n",
    "            result = games_howell_test_task_base(_df=model_split_df[model], task=task, _dependent_var=dependent_var)\n",
    "            if result is not None:\n",
    "                result['model'] = model\n",
    "                result['task'] = task\n",
    "                result['dependent_var'] = dependent_var\n",
    "                result['setup'] = [f\"{x.lower()}-{y.lower()}\" for x,y in zip(result['A'], result['B'])]\n",
    "                result['diff'] = [f\"{float(x):.2e}\" for x in result['diff']]\n",
    "                result['pval'] = [f\"{float(x):.2e}\" for x in result['pval']]\n",
    "                print(result[['diff', 'pval']])\n",
    "                dfs.append(result[['model', 'task', 'setup', 'diff', 'pval', 'dependent_var']])\n",
    "\n",
    "pd.concat(dfs, axis=0).to_csv(\"diff_pval_summary_task_incl_base_8values.csv\", index=False)\n",
    "print(\"output written\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "ee8816c2-3ecf-4fde-90a7-43713afbc9a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output written\n"
     ]
    }
   ],
   "source": [
    "## code to process this output\n",
    "\n",
    "import re\n",
    "import csv\n",
    "\n",
    "equality_text = \"\"\"\n",
    "-------------------- Gemma --------------------\n",
    "     diff      pval\n",
    "0  2.0525  0.000096\n",
    "-------------------- Llama --------------------\n",
    "    diff      pval\n",
    "0 -0.995  0.079584\n",
    "-------------------- Falcon --------------------\n",
    "      diff      pval\n",
    "0 -0.47622  0.259967\n",
    "-------------------- Mistral --------------------\n",
    "       diff          pval\n",
    "0 -6.344743  2.413625e-13\n",
    "\"\"\"\n",
    "\n",
    "nation_text = \"\"\"\n",
    "-------------------- Gemma --------------------\n",
    "      diff  pval\n",
    "0 -3.75875   0.0\n",
    "-------------------- Llama --------------------\n",
    "      diff      pval\n",
    "0 -0.96375  0.063674\n",
    "-------------------- Falcon --------------------\n",
    "       diff      pval\n",
    "0 -0.397142  0.259856\n",
    "-------------------- Mistral --------------------\n",
    "       diff          pval\n",
    "0  5.472634  3.574918e-14\n",
    "\"\"\"\n",
    "\n",
    "liberty_text = \"\"\"\n",
    "-------------------- Gemma --------------------\n",
    "       diff          pval\n",
    "0  4.771563  1.755485e-12\n",
    "-------------------- Llama --------------------\n",
    "       diff      pval\n",
    "0 -0.924375  0.054039\n",
    "-------------------- Falcon --------------------\n",
    "       diff      pval\n",
    "0 -1.054154  0.019261\n",
    "-------------------- Mistral --------------------\n",
    "       diff          pval\n",
    "0 -2.808525  2.591672e-07\n",
    "\"\"\"\n",
    "\n",
    "tradition_text = \"\"\"\n",
    "-------------------- Gemma --------------------\n",
    "       diff      pval\n",
    "0 -1.539062  0.002516\n",
    "-------------------- Llama --------------------\n",
    "       diff      pval\n",
    "0  1.584375  0.000202\n",
    "-------------------- Falcon --------------------\n",
    "      diff          pval\n",
    "0  2.41535  1.158946e-07\n",
    "-------------------- Mistral --------------------\n",
    "       diff  pval\n",
    "0  8.277137   0.0\n",
    "\"\"\"\n",
    "\n",
    "# Step 2: Function to extract (diff, pval) per model\n",
    "def extract_model_data(block_text):\n",
    "    model_blocks = re.split(r'-{5,} (.+?) -{5,}', block_text)\n",
    "    data = {}\n",
    "    for i in range(1, len(model_blocks), 2):\n",
    "        model = model_blocks[i].strip()\n",
    "        block = model_blocks[i + 1]\n",
    "        # Search for a line like: 0  -0.573642  0.000008\n",
    "        match = re.search(r'[-+]?\\d*\\.\\d+(?:[eE][-+]?\\d+)?\\s+[-+]?\\d*\\.\\d+(?:[eE][-+]?\\d+)?', block)\n",
    "        if match:\n",
    "            nums = re.findall(r'[-+]?\\d*\\.\\d+(?:[eE][-+]?\\d+)?', match.group())\n",
    "            if len(nums) == 2:\n",
    "                diff = float(nums[0])\n",
    "                pval = float(nums[1])\n",
    "                data[model] = (f\"{diff:.2e}\", f\"{pval:.2e}\")\n",
    "    return data\n",
    "\n",
    "# Step 3: Extract both x and y\n",
    "liberty_data = extract_model_data(liberty_text)\n",
    "nation_data = extract_model_data(nation_text)\n",
    "equality_data = extract_model_data(equality_text)\n",
    "tradition_data = extract_model_data(tradition_text)\n",
    "\n",
    "# Step 4: Write to CSV\n",
    "with open(\"diff_pval_summary_8values.csv\", \"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"model\", \"diff-liberty\", \"p-value-liberty\", \"diff-nation\", \"p-value-nation\", \"diff-equality\", \"p-value-equality\", \"diff-tradition\", \"p-value-tradition\"])\n",
    "    for model in models:\n",
    "        diff_liberty, pval_liberty = liberty_data.get(model, (\"\", \"\"))\n",
    "        diff_nation, pval_nation = nation_data.get(model, (\"\", \"\"))\n",
    "        diff_equality, pval_equality = equality_data.get(model, (\"\", \"\"))\n",
    "        diff_tradition, pval_tradition = tradition_data.get(model, (\"\", \"\"))\n",
    "        writer.writerow([model, diff_liberty, pval_liberty, diff_nation, pval_nation, diff_equality, pval_equality, diff_tradition, pval_tradition])\n",
    "print(\"output written\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3226d7b7",
   "metadata": {},
   "source": [
    "### Effect of variables after binning the data\n",
    "\n",
    "Since 8 values provide a score like 54% nationalistic and 46% globalist (in the nation scale), we can group the responses into three categories: nationalistic, moderate and globalist (score $\\leq 33.33\\% \\implies$ nationalist,  $33.33\\% \\ge$ score $\\le 66.67\\% \\implies$ moderate and score $\\geq 66.67\\% \\implies$ globalist. Our goal is to see if any of the generation parameters (finetuning, prompting, or decoding parameters) makes a model move from one category to another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65fe50d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2704\n",
      "{'Newsroom', 'Pol-Convo', 'Imdb', 'FineTome', 'Newsarticles', 'Base', 'Canadian-QA', 'OpenR1', 'Scisumm'}\n",
      "2704\n",
      "-------------------- base models --------------------\n",
      "Mistral 80\n",
      "Falcon 80\n",
      "Gemma 80\n",
      "Llama 80\n",
      "=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=: Gemma =:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:\n",
      "equality {'moderate', 'socialist'}\n",
      "nation {'moderate', 'globalist'}\n",
      "liberty {'moderate', 'libertarian'}\n",
      "tradition {'progressive'}\n",
      "=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=: Llama =:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:\n",
      "equality {'socialist'}\n",
      "nation {'moderate', 'globalist'}\n",
      "liberty {'moderate', 'libertarian'}\n",
      "tradition {'moderate', 'progressive'}\n",
      "=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=: Falcon =:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:\n",
      "equality {'moderate', 'socialist'}\n",
      "nation {'moderate'}\n",
      "liberty {'moderate'}\n",
      "tradition {'moderate', 'progressive'}\n",
      "=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=: Mistral =:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:\n",
      "equality {'socialist'}\n",
      "nation {'moderate', 'globalist'}\n",
      "liberty {'moderate', 'libertarian'}\n",
      "tradition {'moderate', 'progressive'}\n",
      "-------------------- fine tuned models --------------------\n",
      "Mistral 544\n",
      "Falcon 560\n",
      "Gemma 640\n",
      "Llama 640\n",
      "=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=: Gemma =:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:\n",
      "equality {'moderate', 'socialist'}\n",
      "nation {'moderate', 'globalist'}\n",
      "liberty {'moderate', 'authoritarian', 'libertarian'}\n",
      "tradition {'moderate', 'progressive'}\n",
      "=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=: Llama =:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:\n",
      "equality {'moderate', 'socialist'}\n",
      "nation {'moderate', 'globalist'}\n",
      "liberty {'moderate', 'libertarian'}\n",
      "tradition {'moderate', 'progressive'}\n",
      "=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=: Falcon =:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:\n",
      "equality {'moderate', 'socialist'}\n",
      "nation {'moderate', 'globalist'}\n",
      "liberty {'moderate', 'libertarian'}\n",
      "tradition {'moderate', 'progressive'}\n",
      "=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=: Mistral =:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:\n",
      "equality {'moderate', 'socialist'}\n",
      "nation {'moderate', 'globalist'}\n",
      "liberty {'libertarian', 'moderate'}\n",
      "tradition {'moderate', 'progressive'}\n",
      "-------------------- base + fine tuned models --------------------\n",
      "Mistral 624\n",
      "Falcon 640\n",
      "Gemma 720\n",
      "Llama 720\n",
      "=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=: Gemma =:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:\n",
      "equality {'moderate', 'socialist'}\n",
      "nation {'moderate', 'globalist'}\n",
      "liberty {'authoritarian', 'moderate', 'libertarian'}\n",
      "tradition {'moderate', 'progressive'}\n",
      "=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=: Llama =:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:\n",
      "equality {'moderate', 'socialist'}\n",
      "nation {'moderate', 'globalist'}\n",
      "liberty {'moderate', 'libertarian'}\n",
      "tradition {'moderate', 'progressive'}\n",
      "=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=: Falcon =:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:\n",
      "equality {'moderate', 'socialist'}\n",
      "nation {'moderate', 'globalist'}\n",
      "liberty {'moderate', 'libertarian'}\n",
      "tradition {'moderate', 'progressive'}\n",
      "=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=: Mistral =:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:\n",
      "equality {'moderate', 'socialist'}\n",
      "nation {'moderate', 'globalist'}\n",
      "liberty {'moderate', 'libertarian'}\n",
      "tradition {'moderate', 'progressive'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6v/k0njs4gd3md5_466d01s4_l80000gq/T/ipykernel_52796/1078701319.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  _df_copy.rename(columns={v:k for k,v in columns.items()}, inplace=True)\n",
      "/var/folders/6v/k0njs4gd3md5_466d01s4_l80000gq/T/ipykernel_52796/1078701319.py:39: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  _df.rename(columns={v:k for k,v in columns.items()}, inplace=True)\n",
      "/var/folders/6v/k0njs4gd3md5_466d01s4_l80000gq/T/ipykernel_52796/1078701319.py:51: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  _df_copy_2.rename(columns={v:k for k,v in columns.items()}, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "## we will start by redownloading the data\n",
    "url = \"https://docs.google.com/spreadsheets/d/1Opmxs3CoUepJTnPSQSwkztpiMnpda5RO7dMt1p-uSjc/edit?gid=0#gid=0\"\n",
    "df = read_gsheet(url=url)\n",
    "from copy import deepcopy\n",
    "print(len(df))\n",
    "df = df.drop_duplicates()\n",
    "df = df[df.model != \"Phi\"]  # filter out Phi\n",
    "print(set(df['fine_tune_dataset']))\n",
    "df['prompt'] = df['prompt'].astype(str)\n",
    "print(len(df))\n",
    "\n",
    "columns = {}\n",
    "columns['equality'] = \"Equality_binned\"\n",
    "columns['nation'] = \"Nation_binned\"\n",
    "columns['liberty'] = \"Liberty_binned\"\n",
    "columns['tradition'] = \"Tradition_binned\"\n",
    "\n",
    "df_copy = deepcopy(df)\n",
    "df_copy_2 = deepcopy(df_copy)\n",
    "models = [\"Gemma\", \"Llama\", \"Falcon\", \"Mistral\"]\n",
    "dependent_vars = [\"equality\", \"nation\", \"liberty\", \"tradition\"]\n",
    "\n",
    "print(\"-\"*20+\" base models \"+\"-\"*20)\n",
    "base_model_split_df = {}\n",
    "for model in set(df['model']):\n",
    "    _df_copy = df_copy[(df_copy['model']== model) & (df_copy['fine_tune_dataset'] == 'Base')]\n",
    "    _df_copy.rename(columns={v:k for k,v in columns.items()}, inplace=True)\n",
    "    print(model, len(_df_copy))\n",
    "    base_model_split_df[model] = _df_copy\n",
    "for model in models:\n",
    "    print(f\"=:\"*20+f\" {model} \"+\"=:\"*20)\n",
    "    for dependent_var in dependent_vars:\n",
    "        print(f\"{dependent_var}\", set(base_model_split_df[model][dependent_var]))\n",
    "\n",
    "print(\"-\"*20+\" fine tuned models \"+\"-\"*20)\n",
    "ft_model_split_df = {}\n",
    "for model in set(df['model']):\n",
    "    _df = df[(df['model']== model) & (df['fine_tune_dataset'] != 'Base')]\n",
    "    _df.rename(columns={v:k for k,v in columns.items()}, inplace=True)\n",
    "    print(model, len(_df))\n",
    "    ft_model_split_df[model] = _df\n",
    "for model in models:\n",
    "    print(f\"=:\"*20+f\" {model} \"+\"=:\"*20)\n",
    "    for dependent_var in dependent_vars:\n",
    "        print(f\"{dependent_var}\", set(ft_model_split_df[model][dependent_var]))\n",
    "\n",
    "print(\"-\"*20+\" base + fine tuned models \"+\"-\"*20)\n",
    "base_ft_model_split_df = {}\n",
    "for model in set(df['model']):\n",
    "    _df_copy_2 = df_copy_2[(df_copy_2['model']== model)]\n",
    "    _df_copy_2.rename(columns={v:k for k,v in columns.items()}, inplace=True)\n",
    "    print(model, len(_df_copy_2))\n",
    "    base_ft_model_split_df[model] = _df_copy_2\n",
    "for model in models:\n",
    "    print(f\"=:\"*20+f\" {model} \"+\"=:\"*20)\n",
    "    for dependent_var in dependent_vars:\n",
    "        print(f\"{dependent_var}\", set(base_ft_model_split_df[model][dependent_var]).union(set(ft_model_split_df[model][dependent_var])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "009b4668-335a-45d0-b94b-f8ac029e0e97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=: Gemma =:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:\n",
      "-------------------- equality --------------------\n",
      "Kruskal Test for equality with n_beams:\n",
      "Kruskal Test: Statistic = 3.7731, p-value = 5.2082e-02\n",
      "Kruskal Test for equality with tmp:\n",
      "Kruskal Test: Statistic = 1.0575, p-value = 3.0378e-01\n",
      "Kruskal Test for equality with top_k:\n",
      "Kruskal Test: Statistic = 0.6397, p-value = 4.2381e-01\n",
      "-------------------- nation --------------------\n",
      "Kruskal Test for nation with n_beams:\n",
      "Kruskal Test: Statistic = 3.6287, p-value = 5.6790e-02\n",
      "Kruskal Test for nation with tmp:\n",
      "Kruskal Test: Statistic = 0.0300, p-value = 8.6251e-01\n",
      "Kruskal Test for nation with top_k:\n",
      "Kruskal Test: Statistic = 0.2699, p-value = 6.0339e-01\n",
      "-------------------- liberty --------------------\n",
      "Kruskal Test for liberty with n_beams:\n",
      "Kruskal Test: Statistic = 2.5280, p-value = 1.1184e-01\n",
      "Kruskal Test for liberty with tmp:\n",
      "Kruskal Test: Statistic = 0.2670, p-value = 6.0535e-01\n",
      "Kruskal Test for liberty with top_k:\n",
      "Kruskal Test: Statistic = 0.0004, p-value = 9.8412e-01\n",
      "-------------------- tradition --------------------\n",
      "Kruskal Test for tradition with n_beams:\n",
      "Kruskal Test: Statistic = 6.9590, p-value = 8.3400e-03\n",
      "Kruskal Test for tradition with tmp:\n",
      "Kruskal Test: Statistic = 2.5052, p-value = 1.1347e-01\n",
      "Kruskal Test for tradition with top_k:\n",
      "Kruskal Test: Statistic = 0.5456, p-value = 4.6013e-01\n",
      "=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=: Llama =:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:\n",
      "-------------------- equality --------------------\n",
      "Kruskal Test for equality with n_beams:\n",
      "Kruskal Test: Statistic = 69.0020, p-value = 9.8364e-17\n",
      "Kruskal Test for equality with tmp:\n",
      "Kruskal Test: Statistic = 16.4193, p-value = 5.0766e-05\n",
      "Kruskal Test for equality with top_k:\n",
      "Kruskal Test: Statistic = 0.0410, p-value = 8.3944e-01\n",
      "-------------------- nation --------------------\n",
      "Kruskal Test for nation with n_beams:\n",
      "Kruskal Test: Statistic = 12.0303, p-value = 5.2342e-04\n",
      "Kruskal Test for nation with tmp:\n",
      "Kruskal Test: Statistic = 4.4574, p-value = 3.4751e-02\n",
      "Kruskal Test for nation with top_k:\n",
      "Kruskal Test: Statistic = 0.0227, p-value = 8.8013e-01\n",
      "-------------------- liberty --------------------\n",
      "Kruskal Test for liberty with n_beams:\n",
      "Kruskal Test: Statistic = 7.2626, p-value = 7.0404e-03\n",
      "Kruskal Test for liberty with tmp:\n",
      "Kruskal Test: Statistic = 0.3335, p-value = 5.6361e-01\n",
      "Kruskal Test for liberty with top_k:\n",
      "Kruskal Test: Statistic = 0.0000, p-value = 1.0000e+00\n",
      "-------------------- tradition --------------------\n",
      "Kruskal Test for tradition with n_beams:\n",
      "Kruskal Test: Statistic = 21.0234, p-value = 4.5371e-06\n",
      "Kruskal Test for tradition with tmp:\n",
      "Kruskal Test: Statistic = 9.7212, p-value = 1.8215e-03\n",
      "Kruskal Test for tradition with top_k:\n",
      "Kruskal Test: Statistic = 0.0336, p-value = 8.5448e-01\n",
      "=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=: Falcon =:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:\n",
      "-------------------- equality --------------------\n",
      "Kruskal Test for equality with n_beams:\n",
      "Kruskal Test: Statistic = 92.3751, p-value = 7.1710e-22\n",
      "Kruskal Test for equality with tmp:\n",
      "Kruskal Test: Statistic = 0.0000, p-value = 1.0000e+00\n",
      "Kruskal Test for equality with top_k:\n",
      "Kruskal Test: Statistic = 0.0000, p-value = 1.0000e+00\n",
      "-------------------- nation --------------------\n",
      "Kruskal Test for nation with n_beams:\n",
      "Kruskal Test: Statistic = 46.3689, p-value = 9.7960e-12\n",
      "Kruskal Test for nation with tmp:\n",
      "Kruskal Test: Statistic = 0.0000, p-value = 1.0000e+00\n",
      "Kruskal Test for nation with top_k:\n",
      "Kruskal Test: Statistic = 0.0000, p-value = 1.0000e+00\n",
      "-------------------- liberty --------------------\n",
      "Kruskal Test for liberty with n_beams:\n",
      "Kruskal Test: Statistic = 25.1935, p-value = 5.1855e-07\n",
      "Kruskal Test for liberty with tmp:\n",
      "Kruskal Test: Statistic = 0.0000, p-value = 1.0000e+00\n",
      "Kruskal Test for liberty with top_k:\n",
      "Kruskal Test: Statistic = 0.0000, p-value = 1.0000e+00\n",
      "-------------------- tradition --------------------\n",
      "Kruskal Test for tradition with n_beams:\n",
      "Kruskal Test: Statistic = 126.6819, p-value = 2.1806e-29\n",
      "Kruskal Test for tradition with tmp:\n",
      "Kruskal Test: Statistic = 0.0000, p-value = 1.0000e+00\n",
      "Kruskal Test for tradition with top_k:\n",
      "Kruskal Test: Statistic = 0.0000, p-value = 1.0000e+00\n",
      "=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=: Mistral =:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:\n",
      "-------------------- equality --------------------\n",
      "Kruskal Test for equality with n_beams:\n",
      "Kruskal Test: Statistic = 0.0874, p-value = 7.6746e-01\n",
      "Kruskal Test for equality with tmp:\n",
      "Kruskal Test: Statistic = 0.0000, p-value = 1.0000e+00\n",
      "Kruskal Test for equality with top_k:\n",
      "Kruskal Test: Statistic = 0.0000, p-value = 1.0000e+00\n",
      "-------------------- nation --------------------\n",
      "Kruskal Test for nation with n_beams:\n",
      "Kruskal Test: Statistic = 0.0286, p-value = 8.6560e-01\n",
      "Kruskal Test for nation with tmp:\n",
      "Kruskal Test: Statistic = 0.0000, p-value = 1.0000e+00\n",
      "Kruskal Test for nation with top_k:\n",
      "Kruskal Test: Statistic = 0.0000, p-value = 1.0000e+00\n",
      "-------------------- liberty --------------------\n",
      "Kruskal Test for liberty with n_beams:\n",
      "Kruskal Test: Statistic = 6.0069, p-value = 1.4250e-02\n",
      "Kruskal Test for liberty with tmp:\n",
      "Kruskal Test: Statistic = 0.0000, p-value = 1.0000e+00\n",
      "Kruskal Test for liberty with top_k:\n",
      "Kruskal Test: Statistic = 0.0000, p-value = 1.0000e+00\n",
      "-------------------- tradition --------------------\n",
      "Kruskal Test for tradition with n_beams:\n",
      "Kruskal Test: Statistic = 0.3648, p-value = 5.4586e-01\n",
      "Kruskal Test for tradition with tmp:\n",
      "Kruskal Test: Statistic = 0.0000, p-value = 1.0000e+00\n",
      "Kruskal Test for tradition with top_k:\n",
      "Kruskal Test: Statistic = 0.0000, p-value = 1.0000e+00\n"
     ]
    }
   ],
   "source": [
    "# first lets do a kruskal test to see if any of the indepndent variables individually afftect the dependent variables\n",
    "from scipy.stats import kruskal\n",
    "independent_vars = ['n_beams', 'tmp', 'top_k']\n",
    "for model in models:\n",
    "    print(\"=:\"*20+f\" {model} \"+\"=:\"*20)\n",
    "    for dependent_var in dependent_vars:\n",
    "        print(\"-\"*20+f\" {dependent_var} \"+\"-\"*20)\n",
    "        # we will do a kruskal test\n",
    "        for independent_var in independent_vars:\n",
    "            print(f\"Kruskal Test for {dependent_var} with {independent_var}:\")\n",
    "            groups = [group[1][dependent_var].values for group in base_ft_model_split_df[model].groupby(independent_var)]\n",
    "            stat, p_value = kruskal(*groups)\n",
    "            print(f\"Kruskal Test: Statistic = {stat:.4f}, p-value = {p_value:.4e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42995e5a",
   "metadata": {},
   "source": [
    "# ordinal logistic regression\n",
    "\n",
    "we will fit an ordinal logistic regression to understand the effect of independent variables on the dependent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "b58c25a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=: Gemma =:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:\n",
      "-------------------- equality --------------------\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.273594\n",
      "         Iterations: 99\n",
      "         Function evaluations: 101\n",
      "         Gradient evaluations: 101\n",
      "                             OrderedModel Results                             \n",
      "==============================================================================\n",
      "Dep. Variable:               equality   Log-Likelihood:                -196.99\n",
      "Model:                   OrderedModel   AIC:                             424.0\n",
      "Method:            Maximum Likelihood   BIC:                             492.7\n",
      "Date:                Sun, 27 Jul 2025                                         \n",
      "Time:                        23:20:55                                         \n",
      "No. Observations:                 720                                         \n",
      "Df Residuals:                     705                                         \n",
      "Df Model:                          14                                         \n",
      "=============================================================================================\n",
      "                                coef    std err          z      P>|z|      [0.025      0.975]\n",
      "---------------------------------------------------------------------------------------------\n",
      "n_beams_5                     0.5633      0.261      2.161      0.031       0.052       1.074\n",
      "tmp_1.0                       0.2978      0.258      1.153      0.249      -0.208       0.804\n",
      "top_k_100                    -0.2316      0.258     -0.898      0.369      -0.737       0.274\n",
      "prompt_1                      1.3856      0.637      2.174      0.030       0.137       2.635\n",
      "prompt_2                      0.4718      0.696      0.678      0.498      -0.891       1.835\n",
      "prompt_3                      2.1439      0.621      3.454      0.001       0.927       3.361\n",
      "prompt_4                      0.4718      0.696      0.678      0.498      -0.891       1.835\n",
      "prompt_5                     -1.4825      1.146     -1.293      0.196      -3.729       0.764\n",
      "prompt_6                      0.6607      0.679      0.973      0.330      -0.670       1.991\n",
      "prompt_7                      1.5053      0.633      2.378      0.017       0.264       2.746\n",
      "prompt_8                      0.4718      0.696      0.678      0.498      -0.891       1.835\n",
      "prompt_9                      1.5053      0.633      2.378      0.017       0.264       2.746\n",
      "fine_tune_dataset_control     0.7166      1.074      0.667      0.505      -1.389       2.822\n",
      "fine_tune_dataset_target      3.4169      1.023      3.340      0.001       1.412       5.422\n",
      "socialist/moderate            5.8139      1.172      4.960      0.000       3.517       8.111\n",
      "=============================================================================================\n",
      "-------------------- nation --------------------\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.110702\n",
      "         Iterations: 77\n",
      "         Function evaluations: 80\n",
      "         Gradient evaluations: 80\n",
      "                             OrderedModel Results                             \n",
      "==============================================================================\n",
      "Dep. Variable:                 nation   Log-Likelihood:                -79.705\n",
      "Model:                   OrderedModel   AIC:                             189.4\n",
      "Method:            Maximum Likelihood   BIC:                             258.1\n",
      "Date:                Sun, 27 Jul 2025                                         \n",
      "Time:                        23:20:56                                         \n",
      "No. Observations:                 720                                         \n",
      "Df Residuals:                     705                                         \n",
      "Df Model:                          14                                         \n",
      "=============================================================================================\n",
      "                                coef    std err          z      P>|z|      [0.025      0.975]\n",
      "---------------------------------------------------------------------------------------------\n",
      "n_beams_5                    -0.8848      0.414     -2.136      0.033      -1.697      -0.073\n",
      "tmp_1.0                      -0.0811      0.401     -0.202      0.840      -0.868       0.705\n",
      "top_k_100                     0.2425      0.402      0.603      0.547      -0.546       1.031\n",
      "prompt_1                     -6.0666   3408.920     -0.002      0.999   -6687.427    6675.293\n",
      "prompt_2                     13.2255    150.737      0.088      0.930    -282.214     308.665\n",
      "prompt_3                     12.7788    150.737      0.085      0.932    -282.661     308.219\n",
      "prompt_4                     -6.0666   3380.843     -0.002      0.999   -6632.397    6620.264\n",
      "prompt_5                     -6.0666   3367.171     -0.002      0.999   -6605.601    6593.468\n",
      "prompt_6                     -6.0666   3361.555     -0.002      0.999   -6594.593    6582.460\n",
      "prompt_7                     -6.0666   3327.143     -0.002      0.999   -6527.146    6515.013\n",
      "prompt_8                     -6.0666   3340.886     -0.002      0.999   -6554.083    6541.949\n",
      "prompt_9                     12.5121    150.737      0.083      0.934    -282.928     307.952\n",
      "fine_tune_dataset_control    -0.9949      0.500     -1.992      0.046      -1.974      -0.016\n",
      "fine_tune_dataset_target     -2.9102      0.674     -4.316      0.000      -4.232      -1.589\n",
      "moderate/globalist           12.8372    150.738      0.085      0.932    -282.603     308.277\n",
      "=============================================================================================\n",
      "-------------------- liberty --------------------\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.420472\n",
      "         Iterations: 110\n",
      "         Function evaluations: 115\n",
      "         Gradient evaluations: 115\n",
      "                             OrderedModel Results                             \n",
      "==============================================================================\n",
      "Dep. Variable:                liberty   Log-Likelihood:                -302.74\n",
      "Model:                   OrderedModel   AIC:                             637.5\n",
      "Method:            Maximum Likelihood   BIC:                             710.7\n",
      "Date:                Sun, 27 Jul 2025                                         \n",
      "Time:                        23:20:56                                         \n",
      "No. Observations:                 720                                         \n",
      "Df Residuals:                     704                                         \n",
      "Df Model:                          14                                         \n",
      "=============================================================================================\n",
      "                                coef    std err          z      P>|z|      [0.025      0.975]\n",
      "---------------------------------------------------------------------------------------------\n",
      "n_beams_5                    -0.4119      0.204     -2.017      0.044      -0.812      -0.012\n",
      "tmp_1.0                       0.1649      0.203      0.811      0.417      -0.233       0.563\n",
      "top_k_100                    -0.0413      0.203     -0.203      0.839      -0.439       0.357\n",
      "prompt_1                    -13.0394    134.155     -0.097      0.923    -275.978     249.899\n",
      "prompt_2                    -15.5058    134.154     -0.116      0.908    -278.444     247.432\n",
      "prompt_3                    -14.4138    134.154     -0.107      0.914    -277.352     248.524\n",
      "prompt_4                    -12.9122    134.155     -0.096      0.923    -275.850     250.026\n",
      "prompt_5                    -14.0588    134.154     -0.105      0.917    -276.997     248.879\n",
      "prompt_6                    -12.7706    134.155     -0.095      0.924    -275.709     250.168\n",
      "prompt_7                    -13.4724    134.154     -0.100      0.920    -276.410     249.466\n",
      "prompt_8                    -12.7706    134.155     -0.095      0.924    -275.709     250.168\n",
      "prompt_9                    -13.9831    134.154     -0.104      0.917    -276.921     248.955\n",
      "fine_tune_dataset_control    -1.6003      0.398     -4.018      0.000      -2.381      -0.820\n",
      "fine_tune_dataset_target     -0.2555      0.409     -0.625      0.532      -1.057       0.546\n",
      "libertarian/moderate        -16.0240    134.155     -0.119      0.905    -278.962     246.914\n",
      "moderate/authoritarian        2.9732      6.860      0.433      0.665     -10.473      16.419\n",
      "=============================================================================================\n",
      "-------------------- tradition --------------------\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.211742\n",
      "         Iterations: 119\n",
      "         Function evaluations: 121\n",
      "         Gradient evaluations: 121\n",
      "                             OrderedModel Results                             \n",
      "==============================================================================\n",
      "Dep. Variable:              tradition   Log-Likelihood:                -152.45\n",
      "Model:                   OrderedModel   AIC:                             334.9\n",
      "Method:            Maximum Likelihood   BIC:                             403.6\n",
      "Date:                Sun, 27 Jul 2025                                         \n",
      "Time:                        23:20:56                                         \n",
      "No. Observations:                 720                                         \n",
      "Df Residuals:                     705                                         \n",
      "Df Model:                          14                                         \n",
      "=============================================================================================\n",
      "                                coef    std err          z      P>|z|      [0.025      0.975]\n",
      "---------------------------------------------------------------------------------------------\n",
      "n_beams_5                    -1.2076      0.335     -3.604      0.000      -1.864      -0.551\n",
      "tmp_1.0                       0.7074      0.315      2.245      0.025       0.090       1.325\n",
      "top_k_100                    -0.3276      0.307     -1.065      0.287      -0.930       0.275\n",
      "prompt_1                     -5.6543      0.760     -7.442      0.000      -7.143      -4.165\n",
      "prompt_2                     -5.6543      0.760     -7.442      0.000      -7.143      -4.165\n",
      "prompt_3                     -6.8083      1.118     -6.092      0.000      -8.999      -4.618\n",
      "prompt_4                     -4.6963      0.617     -7.613      0.000      -5.905      -3.487\n",
      "prompt_5                     -6.0862      0.863     -7.049      0.000      -7.778      -4.394\n",
      "prompt_6                     -5.3391      0.702     -7.610      0.000      -6.714      -3.964\n",
      "prompt_7                     -4.8788      0.637     -7.659      0.000      -6.127      -3.630\n",
      "prompt_8                     -3.2555      0.524     -6.214      0.000      -4.282      -2.229\n",
      "prompt_9                     -6.0862      0.863     -7.049      0.000      -7.778      -4.394\n",
      "fine_tune_dataset_control    13.7557    110.680      0.124      0.901    -203.174     230.685\n",
      "fine_tune_dataset_target     14.5604    110.680      0.132      0.895    -202.369     231.490\n",
      "progressive/moderate         11.3957    110.680      0.103      0.918    -205.533     228.324\n",
      "=============================================================================================\n",
      "=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=: Llama =:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:\n",
      "-------------------- equality --------------------\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.342813\n",
      "         Iterations: 73\n",
      "         Function evaluations: 75\n",
      "         Gradient evaluations: 75\n",
      "                             OrderedModel Results                             \n",
      "==============================================================================\n",
      "Dep. Variable:               equality   Log-Likelihood:                -246.83\n",
      "Model:                   OrderedModel   AIC:                             523.7\n",
      "Method:            Maximum Likelihood   BIC:                             592.3\n",
      "Date:                Sun, 27 Jul 2025                                         \n",
      "Time:                        23:20:56                                         \n",
      "No. Observations:                 720                                         \n",
      "Df Residuals:                     705                                         \n",
      "Df Model:                          14                                         \n",
      "=============================================================================================\n",
      "                                coef    std err          z      P>|z|      [0.025      0.975]\n",
      "---------------------------------------------------------------------------------------------\n",
      "n_beams_5                    -2.1936      0.285     -7.695      0.000      -2.752      -1.635\n",
      "tmp_1.0                       1.0167      0.234      4.340      0.000       0.558       1.476\n",
      "top_k_100                     0.0513      0.226      0.227      0.821      -0.392       0.495\n",
      "prompt_1                      0.6361      0.511      1.246      0.213      -0.365       1.637\n",
      "prompt_2                      0.4024      0.521      0.773      0.440      -0.618       1.423\n",
      "prompt_3                      0.6361      0.511      1.246      0.213      -0.365       1.637\n",
      "prompt_4                      1.0550      0.498      2.117      0.034       0.078       2.032\n",
      "prompt_5                      0.1434      0.535      0.268      0.789      -0.906       1.193\n",
      "prompt_6                      0.1434      0.535      0.268      0.789      -0.906       1.193\n",
      "prompt_7                      0.1434      0.535      0.268      0.789      -0.906       1.193\n",
      "prompt_8                     -0.5153      0.593     -0.870      0.384      -1.677       0.646\n",
      "prompt_9                      0.5221      0.515      1.013      0.311      -0.488       1.532\n",
      "fine_tune_dataset_control    12.0510     78.815      0.153      0.878    -142.423     166.525\n",
      "fine_tune_dataset_target     11.6421     78.815      0.148      0.883    -142.832     166.117\n",
      "socialist/moderate           13.5855     78.816      0.172      0.863    -140.891     168.062\n",
      "=============================================================================================\n",
      "-------------------- nation --------------------\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.627433\n",
      "         Iterations: 51\n",
      "         Function evaluations: 53\n",
      "         Gradient evaluations: 53\n",
      "                             OrderedModel Results                             \n",
      "==============================================================================\n",
      "Dep. Variable:                 nation   Log-Likelihood:                -451.75\n",
      "Model:                   OrderedModel   AIC:                             933.5\n",
      "Method:            Maximum Likelihood   BIC:                             1002.\n",
      "Date:                Sun, 27 Jul 2025                                         \n",
      "Time:                        23:20:57                                         \n",
      "No. Observations:                 720                                         \n",
      "Df Residuals:                     705                                         \n",
      "Df Model:                          14                                         \n",
      "=============================================================================================\n",
      "                                coef    std err          z      P>|z|      [0.025      0.975]\n",
      "---------------------------------------------------------------------------------------------\n",
      "n_beams_5                     0.5786      0.160      3.615      0.000       0.265       0.892\n",
      "tmp_1.0                      -0.3543      0.160     -2.219      0.026      -0.667      -0.041\n",
      "top_k_100                    -0.0255      0.159     -0.160      0.873      -0.338       0.287\n",
      "prompt_1                     -0.3831      0.358     -1.069      0.285      -1.085       0.319\n",
      "prompt_2                     -0.3831      0.358     -1.069      0.285      -1.085       0.319\n",
      "prompt_3                     -0.3170      0.357     -0.889      0.374      -1.016       0.382\n",
      "prompt_4                      0.3044      0.350      0.871      0.384      -0.381       0.990\n",
      "prompt_5                     -0.2519      0.355     -0.709      0.478      -0.948       0.445\n",
      "prompt_6                      0.3648      0.350      1.043      0.297      -0.321       1.050\n",
      "prompt_7                     -0.2519      0.355     -0.709      0.478      -0.948       0.445\n",
      "prompt_8                     -0.2519      0.355     -0.709      0.478      -0.948       0.445\n",
      "prompt_9                      0.1228      0.350      0.351      0.726      -0.564       0.809\n",
      "fine_tune_dataset_control    -1.7100      0.297     -5.760      0.000      -2.292      -1.128\n",
      "fine_tune_dataset_target     -1.9043      0.298     -6.387      0.000      -2.489      -1.320\n",
      "moderate/globalist           -1.2925      0.386     -3.350      0.001      -2.049      -0.536\n",
      "=============================================================================================\n",
      "-------------------- liberty --------------------\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.448588\n",
      "         Iterations: 73\n",
      "         Function evaluations: 75\n",
      "         Gradient evaluations: 75\n",
      "                             OrderedModel Results                             \n",
      "==============================================================================\n",
      "Dep. Variable:                liberty   Log-Likelihood:                -322.98\n",
      "Model:                   OrderedModel   AIC:                             676.0\n",
      "Method:            Maximum Likelihood   BIC:                             744.7\n",
      "Date:                Sun, 27 Jul 2025                                         \n",
      "Time:                        23:20:57                                         \n",
      "No. Observations:                 720                                         \n",
      "Df Residuals:                     705                                         \n",
      "Df Model:                          14                                         \n",
      "=============================================================================================\n",
      "                                coef    std err          z      P>|z|      [0.025      0.975]\n",
      "---------------------------------------------------------------------------------------------\n",
      "n_beams_5                    -0.5516      0.201     -2.742      0.006      -0.946      -0.157\n",
      "tmp_1.0                       0.1181      0.199      0.595      0.552      -0.271       0.507\n",
      "top_k_100                 -8.808e-05      0.198     -0.000      1.000      -0.389       0.389\n",
      "prompt_1                     -0.2393      0.490     -0.488      0.625      -1.200       0.721\n",
      "prompt_2                      0.2884      0.540      0.534      0.593      -0.769       1.346\n",
      "prompt_3                     -0.5390      0.471     -1.145      0.252      -1.462       0.384\n",
      "prompt_4                     -1.0231      0.450     -2.274      0.023      -1.905      -0.141\n",
      "prompt_5                     -0.2393      0.490     -0.488      0.625      -1.200       0.721\n",
      "prompt_6                     -0.4451      0.476     -0.935      0.350      -1.379       0.488\n",
      "prompt_7                     -1.0231      0.450     -2.274      0.023      -1.905      -0.141\n",
      "prompt_8                      0.0006      0.510      0.001      0.999      -0.999       1.000\n",
      "prompt_9                     -0.8744      0.455     -1.921      0.055      -1.766       0.018\n",
      "fine_tune_dataset_control     0.9875      0.308      3.203      0.001       0.383       1.592\n",
      "fine_tune_dataset_target      0.3377      0.292      1.156      0.248      -0.235       0.910\n",
      "libertarian/moderate         -1.6536      0.466     -3.550      0.000      -2.567      -0.741\n",
      "=============================================================================================\n",
      "-------------------- tradition --------------------\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.472089\n",
      "         Iterations: 72\n",
      "         Function evaluations: 74\n",
      "         Gradient evaluations: 74\n",
      "                             OrderedModel Results                             \n",
      "==============================================================================\n",
      "Dep. Variable:              tradition   Log-Likelihood:                -339.90\n",
      "Model:                   OrderedModel   AIC:                             709.8\n",
      "Method:            Maximum Likelihood   BIC:                             778.5\n",
      "Date:                Sun, 27 Jul 2025                                         \n",
      "Time:                        23:20:57                                         \n",
      "No. Observations:                 720                                         \n",
      "Df Residuals:                     705                                         \n",
      "Df Model:                          14                                         \n",
      "=============================================================================================\n",
      "                                coef    std err          z      P>|z|      [0.025      0.975]\n",
      "---------------------------------------------------------------------------------------------\n",
      "n_beams_5                    -0.9122      0.197     -4.619      0.000      -1.299      -0.525\n",
      "tmp_1.0                       0.6205      0.194      3.199      0.001       0.240       1.001\n",
      "top_k_100                     0.0365      0.191      0.191      0.849      -0.338       0.411\n",
      "prompt_1                     -0.0756      0.388     -0.195      0.846      -0.836       0.685\n",
      "prompt_2                     -0.1527      0.391     -0.390      0.696      -0.920       0.614\n",
      "prompt_3                     -0.3171      0.399     -0.794      0.427      -1.100       0.466\n",
      "prompt_4                     -0.3171      0.399     -0.794      0.427      -1.100       0.466\n",
      "prompt_5                     -0.4044      0.404     -1.000      0.317      -1.197       0.388\n",
      "prompt_6                     -1.1835      0.469     -2.522      0.012      -2.103      -0.264\n",
      "prompt_7                     -0.1527      0.391     -0.390      0.696      -0.920       0.614\n",
      "prompt_8                     -1.5119      0.511     -2.957      0.003      -2.514      -0.510\n",
      "prompt_9                     -0.4955      0.410     -1.209      0.227      -1.299       0.308\n",
      "fine_tune_dataset_control     0.7706      0.360      2.142      0.032       0.065       1.476\n",
      "fine_tune_dataset_target      0.3926      0.365      1.076      0.282      -0.322       1.107\n",
      "progressive/moderate          1.4244      0.449      3.169      0.002       0.544       2.305\n",
      "=============================================================================================\n",
      "=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=: Falcon =:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:\n",
      "-------------------- equality --------------------\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.457670\n",
      "         Iterations: 58\n",
      "         Function evaluations: 60\n",
      "         Gradient evaluations: 60\n",
      "                             OrderedModel Results                             \n",
      "==============================================================================\n",
      "Dep. Variable:               equality   Log-Likelihood:                -292.91\n",
      "Model:                   OrderedModel   AIC:                             615.8\n",
      "Method:            Maximum Likelihood   BIC:                             682.7\n",
      "Date:                Sun, 27 Jul 2025                                         \n",
      "Time:                        23:20:57                                         \n",
      "No. Observations:                 640                                         \n",
      "Df Residuals:                     625                                         \n",
      "Df Model:                          14                                         \n",
      "=============================================================================================\n",
      "                                coef    std err          z      P>|z|      [0.025      0.975]\n",
      "---------------------------------------------------------------------------------------------\n",
      "n_beams_5                    -2.0039      0.210     -9.522      0.000      -2.416      -1.591\n",
      "tmp_1.0                   -1.124e-05      0.203  -5.54e-05      1.000      -0.398       0.398\n",
      "top_k_100                 -1.126e-05      0.203  -5.55e-05      1.000      -0.398       0.398\n",
      "prompt_1                     -2.0638      0.615     -3.355      0.001      -3.269      -0.858\n",
      "prompt_2                     -1.2509      0.636     -1.966      0.049      -2.498      -0.004\n",
      "prompt_3                     -2.2313      0.607     -3.673      0.000      -3.422      -1.041\n",
      "prompt_4                     -2.2313      0.607     -3.673      0.000      -3.422      -1.041\n",
      "prompt_5                      7.0703     15.404      0.459      0.646     -23.122      37.262\n",
      "prompt_6                     -2.6139      0.609     -4.292      0.000      -3.808      -1.420\n",
      "prompt_7                     -3.0651      0.612     -5.012      0.000      -4.264      -1.866\n",
      "prompt_8                     -2.4124      0.612     -3.943      0.000      -3.611      -1.213\n",
      "prompt_9                     -2.7430      0.611     -4.491      0.000      -3.940      -1.546\n",
      "fine_tune_dataset_control     0.4498      0.318      1.415      0.157      -0.173       1.073\n",
      "fine_tune_dataset_target      0.7215      0.318      2.266      0.023       0.097       1.346\n",
      "socialist/moderate           -3.4065      0.622     -5.478      0.000      -4.625      -2.188\n",
      "=============================================================================================\n",
      "-------------------- nation --------------------\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.206826\n",
      "         Iterations: 75\n",
      "         Function evaluations: 77\n",
      "         Gradient evaluations: 77\n",
      "                             OrderedModel Results                             \n",
      "==============================================================================\n",
      "Dep. Variable:                 nation   Log-Likelihood:                -132.37\n",
      "Model:                   OrderedModel   AIC:                             294.7\n",
      "Method:            Maximum Likelihood   BIC:                             361.7\n",
      "Date:                Sun, 27 Jul 2025                                         \n",
      "Time:                        23:20:57                                         \n",
      "No. Observations:                 640                                         \n",
      "Df Residuals:                     625                                         \n",
      "Df Model:                          14                                         \n",
      "=============================================================================================\n",
      "                                coef    std err          z      P>|z|      [0.025      0.975]\n",
      "---------------------------------------------------------------------------------------------\n",
      "n_beams_5                     2.5626        nan        nan        nan         nan         nan\n",
      "tmp_1.0                       0.0005        nan        nan        nan         nan         nan\n",
      "top_k_100                     0.0005        nan        nan        nan         nan         nan\n",
      "prompt_1                     15.8078        nan        nan        nan         nan         nan\n",
      "prompt_2                     15.8078        nan        nan        nan         nan         nan\n",
      "prompt_3                     15.7279        nan        nan        nan         nan         nan\n",
      "prompt_4                     15.7279        nan        nan        nan         nan         nan\n",
      "prompt_5                    -47.3676        nan        nan        nan         nan         nan\n",
      "prompt_6                     15.7722        nan        nan        nan         nan         nan\n",
      "prompt_7                     15.8078        nan        nan        nan         nan         nan\n",
      "prompt_8                     14.8611        nan        nan        nan         nan         nan\n",
      "prompt_9                     15.8078        nan        nan        nan         nan         nan\n",
      "fine_tune_dataset_control    40.1753        nan        nan        nan         nan         nan\n",
      "fine_tune_dataset_target     38.2515        nan        nan        nan         nan         nan\n",
      "moderate/globalist           58.7041        nan        nan        nan         nan         nan\n",
      "=============================================================================================\n",
      "-------------------- liberty --------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sagnik.raychoudhury/.pyenv/versions/qaskills/lib/python3.11/site-packages/statsmodels/base/model.py:595: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.073905\n",
      "         Iterations: 79\n",
      "         Function evaluations: 82\n",
      "         Gradient evaluations: 82\n",
      "                             OrderedModel Results                             \n",
      "==============================================================================\n",
      "Dep. Variable:                liberty   Log-Likelihood:                -47.299\n",
      "Model:                   OrderedModel   AIC:                             124.6\n",
      "Method:            Maximum Likelihood   BIC:                             191.5\n",
      "Date:                Sun, 27 Jul 2025                                         \n",
      "Time:                        23:20:58                                         \n",
      "No. Observations:                 640                                         \n",
      "Df Residuals:                     625                                         \n",
      "Df Model:                          14                                         \n",
      "=============================================================================================\n",
      "                                coef    std err          z      P>|z|      [0.025      0.975]\n",
      "---------------------------------------------------------------------------------------------\n",
      "n_beams_5                  -128.3375        nan        nan        nan         nan         nan\n",
      "tmp_1.0                      -0.0005        nan        nan        nan         nan         nan\n",
      "top_k_100                    -0.0005        nan        nan        nan         nan         nan\n",
      "prompt_1                     79.8881        nan        nan        nan         nan         nan\n",
      "prompt_2                     79.8881        nan        nan        nan         nan         nan\n",
      "prompt_3                    -72.0360        nan        nan        nan         nan         nan\n",
      "prompt_4                    -72.9668        nan        nan        nan         nan         nan\n",
      "prompt_5                     77.6952        nan        nan        nan         nan         nan\n",
      "prompt_6                    -72.0352        nan        nan        nan         nan         nan\n",
      "prompt_7                     79.8881        nan        nan        nan         nan         nan\n",
      "prompt_8                    -72.3201        nan        nan        nan         nan         nan\n",
      "prompt_9                     79.8881        nan        nan        nan         nan         nan\n",
      "fine_tune_dataset_control   -15.3206        nan        nan        nan         nan         nan\n",
      "fine_tune_dataset_target    -16.7353        nan        nan        nan         nan         nan\n",
      "libertarian/moderate       -218.4599        nan        nan        nan         nan         nan\n",
      "=============================================================================================\n",
      "-------------------- tradition --------------------\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.408497\n",
      "         Iterations: 63\n",
      "         Function evaluations: 64\n",
      "         Gradient evaluations: 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sagnik.raychoudhury/.pyenv/versions/qaskills/lib/python3.11/site-packages/statsmodels/base/model.py:595: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             OrderedModel Results                             \n",
      "==============================================================================\n",
      "Dep. Variable:              tradition   Log-Likelihood:                -261.44\n",
      "Model:                   OrderedModel   AIC:                             552.9\n",
      "Method:            Maximum Likelihood   BIC:                             619.8\n",
      "Date:                Sun, 27 Jul 2025                                         \n",
      "Time:                        23:20:58                                         \n",
      "No. Observations:                 640                                         \n",
      "Df Residuals:                     625                                         \n",
      "Df Model:                          14                                         \n",
      "=============================================================================================\n",
      "                                coef    std err          z      P>|z|      [0.025      0.975]\n",
      "---------------------------------------------------------------------------------------------\n",
      "n_beams_5                    -2.6577      0.239    -11.129      0.000      -3.126      -2.190\n",
      "tmp_1.0                   -5.024e-05      0.218     -0.000      1.000      -0.426       0.426\n",
      "top_k_100                 -5.024e-05      0.218     -0.000      1.000      -0.426       0.426\n",
      "prompt_1                     -2.9718      0.641     -4.639      0.000      -4.227      -1.716\n",
      "prompt_2                     -2.1665      0.644     -3.367      0.001      -3.428      -0.905\n",
      "prompt_3                     -2.2625      0.636     -3.558      0.000      -3.509      -1.016\n",
      "prompt_4                     -2.2625      0.636     -3.558      0.000      -3.509      -1.016\n",
      "prompt_5                      7.5751     17.708      0.428      0.669     -27.133      42.283\n",
      "prompt_6                     -2.2909      0.638     -3.594      0.000      -3.540      -1.041\n",
      "prompt_7                     -2.9718      0.641     -4.639      0.000      -4.227      -1.716\n",
      "prompt_8                     -3.3502      0.642     -5.216      0.000      -4.609      -2.091\n",
      "prompt_9                     -2.5803      0.641     -4.027      0.000      -3.836      -1.324\n",
      "fine_tune_dataset_control    -1.1439      0.394     -2.905      0.004      -1.916      -0.372\n",
      "fine_tune_dataset_target     -2.2908      0.401     -5.710      0.000      -3.077      -1.505\n",
      "progressive/moderate         -5.9792      0.731     -8.181      0.000      -7.412      -4.547\n",
      "=============================================================================================\n",
      "=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=: Mistral =:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:=:\n",
      "-------------------- equality --------------------\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.552259\n",
      "         Iterations: 54\n",
      "         Function evaluations: 56\n",
      "         Gradient evaluations: 56\n",
      "                             OrderedModel Results                             \n",
      "==============================================================================\n",
      "Dep. Variable:               equality   Log-Likelihood:                -344.61\n",
      "Model:                   OrderedModel   AIC:                             719.2\n",
      "Method:            Maximum Likelihood   BIC:                             785.8\n",
      "Date:                Sun, 27 Jul 2025                                         \n",
      "Time:                        23:20:58                                         \n",
      "No. Observations:                 624                                         \n",
      "Df Residuals:                     609                                         \n",
      "Df Model:                          14                                         \n",
      "=============================================================================================\n",
      "                                coef    std err          z      P>|z|      [0.025      0.975]\n",
      "---------------------------------------------------------------------------------------------\n",
      "n_beams_5                     0.0923      0.183      0.505      0.613      -0.266       0.450\n",
      "tmp_1.0                   -1.357e-06      0.182  -7.44e-06      1.000      -0.357       0.357\n",
      "top_k_100                 -1.354e-06      0.182  -7.43e-06      1.000      -0.357       0.357\n",
      "prompt_1                      1.1081      0.409      2.711      0.007       0.307       1.909\n",
      "prompt_2                      1.2320      0.405      3.040      0.002       0.438       2.026\n",
      "prompt_3                      1.1081      0.409      2.711      0.007       0.307       1.909\n",
      "prompt_4                      0.4353      0.393      1.109      0.268      -0.334       1.205\n",
      "prompt_5                      1.3081      0.402      3.253      0.001       0.520       2.096\n",
      "prompt_6                      0.4353      0.393      1.109      0.268      -0.334       1.205\n",
      "prompt_7                      0.2954      0.385      0.768      0.443      -0.459       1.049\n",
      "prompt_8                     -0.6430      0.405     -1.587      0.112      -1.437       0.151\n",
      "prompt_9                      0.1190      0.393      0.303      0.762      -0.651       0.889\n",
      "fine_tune_dataset_control    16.6613    315.087      0.053      0.958    -600.899     634.221\n",
      "fine_tune_dataset_target     16.0307    315.087      0.051      0.959    -601.529     633.590\n",
      "socialist/moderate           16.6413    315.087      0.053      0.958    -600.919     634.201\n",
      "=============================================================================================\n",
      "-------------------- nation --------------------\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.491293\n",
      "         Iterations: 66\n",
      "         Function evaluations: 68\n",
      "         Gradient evaluations: 68\n",
      "                             OrderedModel Results                             \n",
      "==============================================================================\n",
      "Dep. Variable:                 nation   Log-Likelihood:                -306.57\n",
      "Model:                   OrderedModel   AIC:                             643.1\n",
      "Method:            Maximum Likelihood   BIC:                             709.7\n",
      "Date:                Sun, 27 Jul 2025                                         \n",
      "Time:                        23:20:58                                         \n",
      "No. Observations:                 624                                         \n",
      "Df Residuals:                     609                                         \n",
      "Df Model:                          14                                         \n",
      "=============================================================================================\n",
      "                                coef    std err          z      P>|z|      [0.025      0.975]\n",
      "---------------------------------------------------------------------------------------------\n",
      "n_beams_5                     0.0221      0.199      0.111      0.912      -0.368       0.412\n",
      "tmp_1.0                   -5.778e-05      0.199     -0.000      1.000      -0.389       0.389\n",
      "top_k_100                 -5.779e-05      0.199     -0.000      1.000      -0.389       0.389\n",
      "prompt_1                      2.4198      0.589      4.109      0.000       1.266       3.574\n",
      "prompt_2                      0.7780      0.646      1.204      0.229      -0.488       2.044\n",
      "prompt_3                      2.4198      0.589      4.109      0.000       1.266       3.574\n",
      "prompt_4                      2.4198      0.589      4.109      0.000       1.266       3.574\n",
      "prompt_5                     -0.0303      0.735     -0.041      0.967      -1.471       1.410\n",
      "prompt_6                      2.4198      0.589      4.109      0.000       1.266       3.574\n",
      "prompt_7                      1.6590      0.599      2.771      0.006       0.485       2.833\n",
      "prompt_8                      1.6590      0.599      2.771      0.006       0.485       2.833\n",
      "prompt_9                      1.7716      0.601      2.947      0.003       0.593       2.950\n",
      "fine_tune_dataset_control    -1.4780      0.310     -4.761      0.000      -2.086      -0.870\n",
      "fine_tune_dataset_target     -0.4221      0.280     -1.505      0.132      -0.972       0.128\n",
      "moderate/globalist            2.0843      0.589      3.541      0.000       0.931       3.238\n",
      "=============================================================================================\n",
      "-------------------- liberty --------------------\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.630937\n",
      "         Iterations: 51\n",
      "         Function evaluations: 53\n",
      "         Gradient evaluations: 53\n",
      "                             OrderedModel Results                             \n",
      "==============================================================================\n",
      "Dep. Variable:                liberty   Log-Likelihood:                -393.70\n",
      "Model:                   OrderedModel   AIC:                             817.4\n",
      "Method:            Maximum Likelihood   BIC:                             884.0\n",
      "Date:                Sun, 27 Jul 2025                                         \n",
      "Time:                        23:20:58                                         \n",
      "No. Observations:                 624                                         \n",
      "Df Residuals:                     609                                         \n",
      "Df Model:                          14                                         \n",
      "=============================================================================================\n",
      "                                coef    std err          z      P>|z|      [0.025      0.975]\n",
      "---------------------------------------------------------------------------------------------\n",
      "n_beams_5                    -0.4598      0.171     -2.685      0.007      -0.795      -0.124\n",
      "tmp_1.0                    1.946e-05      0.171      0.000      1.000      -0.334       0.334\n",
      "top_k_100                  1.948e-05      0.171      0.000      1.000      -0.334       0.334\n",
      "prompt_1                     -1.3303      0.400     -3.329      0.001      -2.114      -0.547\n",
      "prompt_2                     -0.3276      0.406     -0.807      0.420      -1.123       0.468\n",
      "prompt_3                     -1.3303      0.400     -3.329      0.001      -2.114      -0.547\n",
      "prompt_4                     -0.7570      0.402     -1.881      0.060      -1.546       0.032\n",
      "prompt_5                     -0.8257      0.391     -2.112      0.035      -1.592      -0.059\n",
      "prompt_6                     -1.0464      0.399     -2.620      0.009      -1.829      -0.264\n",
      "prompt_7                     -0.3276      0.406     -0.807      0.420      -1.123       0.468\n",
      "prompt_8                     -1.4303      0.395     -3.625      0.000      -2.204      -0.657\n",
      "prompt_9                     -1.3303      0.400     -3.329      0.001      -2.114      -0.547\n",
      "fine_tune_dataset_control     1.4212      0.281      5.053      0.000       0.870       1.972\n",
      "fine_tune_dataset_target      0.8265      0.270      3.056      0.002       0.296       1.357\n",
      "libertarian/moderate         -0.4424      0.396     -1.117      0.264      -1.218       0.334\n",
      "=============================================================================================\n",
      "-------------------- tradition --------------------\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.480834\n",
      "         Iterations: 59\n",
      "         Function evaluations: 60\n",
      "         Gradient evaluations: 60\n",
      "                             OrderedModel Results                             \n",
      "==============================================================================\n",
      "Dep. Variable:              tradition   Log-Likelihood:                -300.04\n",
      "Model:                   OrderedModel   AIC:                             630.1\n",
      "Method:            Maximum Likelihood   BIC:                             696.6\n",
      "Date:                Sun, 27 Jul 2025                                         \n",
      "Time:                        23:20:59                                         \n",
      "No. Observations:                 624                                         \n",
      "Df Residuals:                     609                                         \n",
      "Df Model:                          14                                         \n",
      "=============================================================================================\n",
      "                                coef    std err          z      P>|z|      [0.025      0.975]\n",
      "---------------------------------------------------------------------------------------------\n",
      "n_beams_5                    -0.2517      0.203     -1.240      0.215      -0.649       0.146\n",
      "tmp_1.0                       0.0003      0.202      0.001      0.999      -0.395       0.396\n",
      "top_k_100                     0.0003      0.202      0.001      0.999      -0.395       0.396\n",
      "prompt_1                      0.0803      0.436      0.184      0.854      -0.775       0.935\n",
      "prompt_2                      1.5024      0.447      3.363      0.001       0.627       2.378\n",
      "prompt_3                     -0.3119      0.437     -0.713      0.476      -1.169       0.545\n",
      "prompt_4                      0.4737      0.438      1.083      0.279      -0.384       1.331\n",
      "prompt_5                      1.9556      0.465      4.203      0.000       1.044       2.868\n",
      "prompt_6                     -0.7065      0.440     -1.604      0.109      -1.570       0.157\n",
      "prompt_7                      0.3675      0.429      0.857      0.392      -0.473       1.208\n",
      "prompt_8                     -2.0174      0.476     -4.242      0.000      -2.950      -1.085\n",
      "prompt_9                     -1.1094      0.447     -2.484      0.013      -1.985      -0.234\n",
      "fine_tune_dataset_control     1.0834      0.311      3.487      0.000       0.475       1.692\n",
      "fine_tune_dataset_target     -1.7347      0.301     -5.756      0.000      -2.325      -1.144\n",
      "progressive/moderate         -0.6147      0.422     -1.458      0.145      -1.441       0.212\n",
      "=============================================================================================\n"
     ]
    }
   ],
   "source": [
    "from numpy import mod\n",
    "from statsmodels.miscmodels.ordinal_model import OrderedModel\n",
    "from copy import deepcopy\n",
    "\n",
    "def get_ordinal_regression_results(_df, _dependent_var, _independent_vars: list, ordinal_categories: list|str):\n",
    "    for _var in _independent_vars:\n",
    "        _df[_var] = _df[_var].astype('category')\n",
    "    X = pd.get_dummies(_df[_independent_vars], drop_first=True)\n",
    "    #X = _df[_independent_vars]\n",
    "    if ordinal_categories is not None and isinstance(ordinal_categories, str):\n",
    "        ordinal_categories = [x.strip() for x in ordinal_categories.split(',')]\n",
    "    if len(ordinal_categories) == 1:\n",
    "        print(f\"Warning: Only one category provided for {ordinal_categories}, returning\")\n",
    "        return\n",
    "    assert set(_df[_dependent_var]).issubset(set(ordinal_categories)), \"Dependent variable categories must be a subset of ordinal categories\"\n",
    "    _df[_dependent_var] = pd.Categorical(_df[_dependent_var], categories=ordinal_categories, ordered=True)\n",
    "    mod = OrderedModel(_df[_dependent_var], X, distr='logit')  # or distr='probit'\n",
    "    res = mod.fit(method='bfgs')\n",
    "    return res.summary()\n",
    "_independent_vars = ['n_beams', 'tmp', 'top_k', \"prompt\", \"fine_tune_dataset\"]\n",
    "models = [\"Gemma\", \"Llama\", \"Falcon\", \"Mistral\"]\n",
    "dependent_var_categories_base = {\n",
    "    'equality': {\n",
    "        'Gemma': 'socialist, moderate',\n",
    "        'Llama': 'socialist, moderate',\n",
    "        'Falcon': 'socialist, moderate',\n",
    "        'Mistral': 'socialist, moderate'\n",
    "    },\n",
    "    'nation': {\n",
    "        'Gemma': 'moderate, globalist',\n",
    "        'Llama': 'moderate, globalist',\n",
    "        'Falcon': 'moderate, globalist',\n",
    "        'Mistral': 'moderate, globalist'\n",
    "    },\n",
    "    'liberty': {\n",
    "        'Gemma': 'libertarian, moderate, authoritarian',\n",
    "        'Llama': 'libertarian, moderate',\n",
    "        'Falcon': 'libertarian, moderate',\n",
    "        'Mistral': 'libertarian, moderate'\n",
    "    },\n",
    "    'tradition': {\n",
    "        'Gemma': 'progressive, moderate',\n",
    "        'Llama': 'progressive, moderate',\n",
    "        'Falcon': 'progressive, moderate',\n",
    "        'Mistral': 'progressive, moderate'\n",
    "    }\n",
    "}\n",
    "\n",
    "dataset_mappings = {\n",
    "    'Newsroom': 'target',\n",
    "    'Pol-Convo': 'target',\n",
    "    'Canadian-QA': 'target',\n",
    "    'Newsarticles': 'target',\n",
    "    'FineTome': 'control',\n",
    "    'OpenR1': 'control',\n",
    "    'Scisumm': 'control',\n",
    "    'Imdb': 'control',\n",
    "    'Base': 'Base'\n",
    " }\n",
    "for model in models:\n",
    "    print(\"=:\"*20+f\" {model} \"+\"=:\"*20)\n",
    "    for dependent_var in dependent_vars:\n",
    "        print(\"-\"*20+f\" {dependent_var} \"+\"-\"*20)\n",
    "        _df = deepcopy(base_ft_model_split_df[model])\n",
    "        mapping = {'Base': 'Base'}\n",
    "        _df.loc[:, 'fine_tune_dataset'] = _df['fine_tune_dataset'].map(dataset_mappings)\n",
    "        assert set(_df['fine_tune_dataset']) == set(dataset_mappings.values()), f\"Fine-tune dataset categories must be a subset of dataset mappings, {set(_df['fine_tune_dataset'])} != {set(dataset_mappings.values())}\"\n",
    "        results = get_ordinal_regression_results(_df, dependent_var, _independent_vars, ordinal_categories=dependent_var_categories_base[dependent_var][model])\n",
    "        print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "1e3e8186",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.heatmap(df[['n_beams', 'tmp', 'top_k']].corr(), annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96d71b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
